{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":394223,"sourceType":"datasetVersion","datasetId":174616}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/Tanveer2719/NIDS_Coursework.git\n%cd /kaggle/working/NIDS_Coursework/My_Code\n!ls /kaggle/input\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-27T14:14:39.563982Z","iopub.execute_input":"2025-08-27T14:14:39.564170Z","iopub.status.idle":"2025-08-27T14:14:40.590206Z","shell.execute_reply.started":"2025-08-27T14:14:39.564154Z","shell.execute_reply":"2025-08-27T14:14:40.589354Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'NIDS_Coursework'...\nremote: Enumerating objects: 262, done.\u001b[K\nremote: Counting objects: 100% (262/262), done.\u001b[K\nremote: Compressing objects: 100% (171/171), done.\u001b[K\nremote: Total 262 (delta 126), reused 221 (delta 85), pack-reused 0 (from 0)\u001b[K\nReceiving objects: 100% (262/262), 1.13 MiB | 11.92 MiB/s, done.\nResolving deltas: 100% (126/126), done.\n/kaggle/working/NIDS_Coursework/My_Code\nnslkdd\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport random\nimport numpy as np\nimport tensorflow as tf\n\ndef set_seed(seed=42):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    tf.keras.utils.set_random_seed(seed)\n    tf.config.experimental.enable_op_determinism()\n\nset_seed(42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T14:14:40.592261Z","iopub.execute_input":"2025-08-27T14:14:40.592534Z","iopub.status.idle":"2025-08-27T14:14:54.118843Z","shell.execute_reply.started":"2025-08-27T14:14:40.592503Z","shell.execute_reply":"2025-08-27T14:14:54.118066Z"}},"outputs":[{"name":"stderr","text":"2025-08-27 14:14:42.439101: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1756304082.648366      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1756304082.715734      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nurl_train = \"/kaggle/input/nslkdd/KDDTrain+.txt\"\nurl_test = \"/kaggle/input/nslkdd/KDDTest+.txt\"\ncolumns = [\n    'duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes',\n    'land', 'wrong_fragment', 'urgent', 'hot', 'num_failed_logins',\n    'logged_in', 'num_compromised', 'root_shell', 'su_attempted', 'num_root',\n    'num_file_creations', 'num_shells', 'num_access_files', 'num_outbound_cmds',\n    'is_host_login', 'is_guest_login', 'count', 'srv_count', 'serror_rate',\n    'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate', 'same_srv_rate',\n    'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count',\n    'dst_host_srv_count', 'dst_host_same_srv_rate', 'dst_host_diff_srv_rate',\n    'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate',\n    'dst_host_serror_rate', 'dst_host_srv_serror_rate', 'dst_host_rerror_rate',\n    'dst_host_srv_rerror_rate', 'attack', 'level'\n]\n\ntrain_df = pd.read_csv(url_train, names=columns)\ntest_df = pd.read_csv(url_test, names=columns)\n\n\n# Drop the \"type\" column (not needed)\ntrain_df.drop(columns=['level'], inplace=True)\ntest_df.drop(columns=['level'], inplace=True)\n\n# Strip whitespace from class labels\nfor df in [train_df, test_df]:\n    df['attack'] = df['attack'].str.strip()\n\n\n\n# Combine train and test\nfull_df = pd.concat([train_df, test_df], ignore_index=True)\n\nfull_df['label'] = full_df['attack'].apply(lambda x: 0 if x == 'normal' else 1)\nfull_df = full_df.drop(columns=['attack'])\n\nprint(f'full_df.shape : {full_df.shape}')\n\n\n\n\n# --------------------------- perform preprocessing--------------\n\nfrom preprocess import Preprocess\n\n# separate the features and labels so that the labesl are not encoded\nlabels = full_df['label']\nfeatures = full_df.drop(columns=['label'])\n\npp = Preprocess()\nprocessed_features = pp.fit_transform_df_auto(df = features,n_categorical_levels=32, expected_categorical_format='onehot')\n\nprint(processed_features.shape)\n\n# concat the features and the labels\nprocessed_full_df = pd.concat([processed_features, labels], axis=1)\n\n# Filter out categorical one-hot encoded columns\nnumerical_columns = [col for col in processed_full_df \n                     if not (col.startswith(\"proto_\") or \n                             col.startswith(\"service_\") or \n                             col.startswith(\"state_\") or\n                             col == 'label')]\n\ntarget = processed_full_df['label']\nfeatures = processed_full_df.drop(columns=['label'])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T14:14:54.119686Z","iopub.execute_input":"2025-08-27T14:14:54.120195Z","iopub.status.idle":"2025-08-27T14:14:55.864257Z","shell.execute_reply.started":"2025-08-27T14:14:54.120169Z","shell.execute_reply":"2025-08-27T14:14:55.863582Z"}},"outputs":[{"name":"stdout","text":"full_df.shape : (148517, 42)\nEncoding the 3 levels for protocol_type\nEncoding the 32 levels for service\nEncoding the 11 levels for flag\n(148517, 84)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import tensorflow as tf\nfrom sklearn.model_selection import train_test_split\n\nX = features.astype(\"float32\").values  # (257673, 95) as float32\ny = target.astype(\"int32\").values      # (257673,)\n\n\n# reshape for (samples, timesteps, features=1)\nX = np.expand_dims(X, axis=-1)  # (257673, 95, 1)\n\n# Split train, val, test\nX_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\nX_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.2, random_state=42, stratify=y_temp)\n\nprint(f\"X_train: {X_train.shape}\")\nprint(f\"y_train: {y_train.shape}\")\nprint(f\"X_val: {X_val.shape}\")\nprint(f\"y_val: {y_val.shape}\")\nprint(f\"X_test: {X_test.shape}\")\nprint(f\"y_test: {y_test.shape}\")\n\n\n\ninput_shape = (X_train.shape[1], X_train.shape[2])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T14:14:55.865022Z","iopub.execute_input":"2025-08-27T14:14:55.865319Z","iopub.status.idle":"2025-08-27T14:14:56.105327Z","shell.execute_reply.started":"2025-08-27T14:14:55.865300Z","shell.execute_reply":"2025-08-27T14:14:56.104598Z"}},"outputs":[{"name":"stdout","text":"X_train: (118813, 84, 1)\ny_train: (118813,)\nX_val: (23763, 84, 1)\ny_val: (23763,)\nX_test: (5941, 84, 1)\ny_test: (5941,)\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n\ndef evaluate_metrics(y_true, y_pred):\n    f1 = f1_score(y_true, y_pred)\n    precision = precision_score(y_true, y_pred)\n    recall = recall_score(y_true, y_pred)\n    cm = confusion_matrix(y_true, y_pred)\n\n    if cm.shape == (2, 2):\n        tn, fp, fn, tp = cm.ravel()\n        numerator = tp * tn - fp * fn\n        denominator = ((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)) ** 0.5\n        mcc = numerator / (denominator + 1e-10)\n        specificity = tn / (tn + fp + 1e-10)\n        balanced_acc = (recall + specificity) / 2\n    else:\n        mcc = 0\n        balanced_acc = 0\n        specificity = 0\n\n    return {\n        'f1': f1,\n        'precision': precision,\n        'recall': recall,\n        'mcc': mcc,\n        'specificity': specificity,\n        'balanced_accuracy': balanced_acc\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T14:14:56.106107Z","iopub.execute_input":"2025-08-27T14:14:56.106349Z","iopub.status.idle":"2025-08-27T14:14:56.112018Z","shell.execute_reply.started":"2025-08-27T14:14:56.106319Z","shell.execute_reply":"2025-08-27T14:14:56.111357Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"## Model Building","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras import layers, models, optimizers, losses, metrics\n\n\nmodel = models.Sequential([\n    layers.Conv1D(32, kernel_size=3, activation='relu', input_shape=input_shape),\n    layers.MaxPooling1D(pool_size=2),\n    layers.Conv1D(64, kernel_size=3, activation='relu'),\n    layers.GlobalMaxPooling1D(),\n    layers.Dense(64, activation='relu'),\n    layers.Dense(1, activation='sigmoid')\n])\n    \n\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n    \nmodel.fit(\n    X_train, y_train,\n    epochs=20,\n    batch_size=256,\n    validation_data=(X_val, y_val),\n    verbose=1\n)\n\ny_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n\nnormal_results = evaluate_metrics(y_true = y_test, y_pred = y_pred)\nprint(f'normal results: {normal_results}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T14:14:56.112673Z","iopub.execute_input":"2025-08-27T14:14:56.112948Z","iopub.status.idle":"2025-08-27T14:15:51.437057Z","shell.execute_reply.started":"2025-08-27T14:14:56.112925Z","shell.execute_reply":"2025-08-27T14:15:51.436367Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\nI0000 00:00:1756304097.121789      36 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\nI0000 00:00:1756304097.122436      36 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/20\n","output_type":"stream"},{"name":"stderr","text":"2025-08-27 14:14:58.538268: E tensorflow/core/framework/node_def_util.cc:676] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_15}}\nI0000 00:00:1756304100.459207     108 cuda_dnn.cc:529] Loaded cuDNN version 90300\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8660 - loss: 0.3518","output_type":"stream"},{"name":"stderr","text":"2025-08-27 14:15:04.427480: E tensorflow/core/framework/node_def_util.cc:676] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_15}}\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.8661 - loss: 0.3515 - val_accuracy: 0.9479 - val_loss: 0.1190\nEpoch 2/20\n\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9535 - loss: 0.1081 - val_accuracy: 0.9683 - val_loss: 0.0838\nEpoch 3/20\n\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9689 - loss: 0.0823 - val_accuracy: 0.9718 - val_loss: 0.0736\nEpoch 4/20\n\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9734 - loss: 0.0720 - val_accuracy: 0.9757 - val_loss: 0.0657\nEpoch 5/20\n\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9753 - loss: 0.0655 - val_accuracy: 0.9756 - val_loss: 0.0637\nEpoch 6/20\n\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9772 - loss: 0.0612 - val_accuracy: 0.9767 - val_loss: 0.0606\nEpoch 7/20\n\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9786 - loss: 0.0579 - val_accuracy: 0.9778 - val_loss: 0.0564\nEpoch 8/20\n\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9796 - loss: 0.0553 - val_accuracy: 0.9789 - val_loss: 0.0527\nEpoch 9/20\n\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9806 - loss: 0.0532 - val_accuracy: 0.9815 - val_loss: 0.0493\nEpoch 10/20\n\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9816 - loss: 0.0511 - val_accuracy: 0.9830 - val_loss: 0.0474\nEpoch 11/20\n\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9824 - loss: 0.0492 - val_accuracy: 0.9841 - val_loss: 0.0460\nEpoch 12/20\n\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9827 - loss: 0.0478 - val_accuracy: 0.9847 - val_loss: 0.0450\nEpoch 13/20\n\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9835 - loss: 0.0466 - val_accuracy: 0.9850 - val_loss: 0.0434\nEpoch 14/20\n\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9836 - loss: 0.0456 - val_accuracy: 0.9848 - val_loss: 0.0430\nEpoch 15/20\n\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9840 - loss: 0.0447 - val_accuracy: 0.9853 - val_loss: 0.0424\nEpoch 16/20\n\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9846 - loss: 0.0437 - val_accuracy: 0.9856 - val_loss: 0.0421\nEpoch 17/20\n\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9848 - loss: 0.0429 - val_accuracy: 0.9855 - val_loss: 0.0409\nEpoch 18/20\n\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9851 - loss: 0.0419 - val_accuracy: 0.9854 - val_loss: 0.0408\nEpoch 19/20\n\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9857 - loss: 0.0412 - val_accuracy: 0.9856 - val_loss: 0.0405\nEpoch 20/20\n\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9859 - loss: 0.0405 - val_accuracy: 0.9862 - val_loss: 0.0401\n\u001b[1m 73/186\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step ","output_type":"stream"},{"name":"stderr","text":"2025-08-27 14:15:50.880384: E tensorflow/core/framework/node_def_util.cc:676] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_14}}\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\nnormal results: {'f1': 0.988429172510519, 'precision': 0.9908611599297013, 'recall': 0.9860090940888423, 'mcc': 0.9777571211186512, 'specificity': 0.9915639195327387, 'balanced_accuracy': 0.9887865068107905}\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"## Attack codes","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\n\n@tf.function\ndef fgsm_attack_tf(model, inputs, labels, epsilon=0.05, clip_min=-1.5, clip_max=1.5):\n    inputs = tf.convert_to_tensor(inputs, dtype=tf.float32)\n    labels = tf.reshape(tf.convert_to_tensor(labels, dtype=tf.float32), (-1, 1))\n\n    with tf.GradientTape() as tape:\n        tape.watch(inputs)\n        predictions = model(inputs, training=False)\n        loss = tf.keras.losses.binary_crossentropy(labels, predictions)\n        loss = tf.reduce_mean(loss)\n\n    gradients = tape.gradient(loss, inputs)\n    signed_grad = tf.sign(gradients)\n    x_adv = inputs + epsilon * signed_grad\n    x_adv = tf.clip_by_value(x_adv, clip_min, clip_max)\n    return x_adv\n\n\n@tf.function\ndef pgd_attack_tf(model, inputs, labels, epsilon=0.05, alpha=0.01, num_iter=10, clip_min=-1.5, clip_max=1.5):\n    inputs = tf.convert_to_tensor(inputs, dtype=tf.float32)\n    labels = tf.reshape(tf.convert_to_tensor(labels, dtype=tf.float32), (-1, 1))\n\n    x_adv = tf.identity(inputs)\n\n    for _ in tf.range(num_iter):\n        with tf.GradientTape() as tape:\n            tape.watch(x_adv)\n            predictions = model(x_adv, training=False)\n            loss = tf.keras.losses.binary_crossentropy(labels, predictions)\n            loss = tf.reduce_mean(loss)\n\n        gradients = tape.gradient(loss, x_adv)\n        x_adv = x_adv + alpha * tf.sign(gradients)\n        x_adv = tf.clip_by_value(x_adv, inputs - epsilon, inputs + epsilon)\n        x_adv = tf.clip_by_value(x_adv, clip_min, clip_max)\n\n    return x_adv\n\n\n@tf.function\ndef mi_fgsm_attack(model, inputs, labels, epsilon=0.05, alpha=0.01, steps=40, decay=1.0, clip_min=-1.5, clip_max=1.5):\n    inputs = tf.convert_to_tensor(inputs, dtype=tf.float32)\n    labels = tf.reshape(tf.convert_to_tensor(labels, dtype=tf.float32), (-1, 1))\n\n    x_adv = tf.identity(inputs)\n    g = tf.zeros_like(inputs)\n\n    for _ in tf.range(steps):\n        with tf.GradientTape() as tape:\n            tape.watch(x_adv)\n            logits = model(x_adv, training=False)\n            loss = tf.keras.losses.binary_crossentropy(labels, logits)\n            loss = tf.reduce_mean(loss)\n\n        grad = tape.gradient(loss, x_adv)\n        # Normalize gradient\n        grad_norm = tf.reduce_mean(tf.abs(grad), axis=list(range(1, len(grad.shape))), keepdims=True)\n        grad = grad / (grad_norm + 1e-8)\n\n        g = decay * g + grad\n        x_adv = x_adv + alpha * tf.sign(g)\n        x_adv = tf.clip_by_value(x_adv, inputs - epsilon, inputs + epsilon)\n        x_adv = tf.clip_by_value(x_adv, clip_min, clip_max)\n\n    return x_adv\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T14:15:51.439004Z","iopub.execute_input":"2025-08-27T14:15:51.439209Z","iopub.status.idle":"2025-08-27T14:15:51.450223Z","shell.execute_reply.started":"2025-08-27T14:15:51.439194Z","shell.execute_reply":"2025-08-27T14:15:51.449603Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def evaluate_model(model, X, y,epsilon_list=[0.01, 0.03, 0.05, 0.07]):\n    import tensorflow as tf\n    import numpy as np\n\n    X_clean = X\n    x_clean_tf = tf.convert_to_tensor(X_clean, dtype=tf.float32)\n    y_tf = tf.convert_to_tensor(y, dtype=tf.float32)\n\n    # ---------------- Clean Evaluation ----------------\n    print(\"✅ Clean Evaluation:\")\n    y_pred_clean = (model.predict(x_clean_tf) > 0.5).astype(int).flatten()\n    clean_metrics = evaluate_metrics(y, y_pred_clean)\n    print(f\"  F1: {clean_metrics['f1']:.4f}, Precision: {clean_metrics['precision']:.4f}, Recall: {clean_metrics['recall']:.4f}, MCC: {clean_metrics.get('mcc', 'N/A')}, Specificity: {clean_metrics.get('specificity', 'N/A')}, Balanced Acc: {clean_metrics.get('balanced_accuracy', 'N/A')}\")\n\n    # ---------------- FGSM Evaluation ----------------\n    print(\"\\nFGSM Attack Evaluation\")\n    fgsm_results = []\n    for eps in epsilon_list:\n        x_adv = fgsm_attack_tf(model, x_clean_tf, y_tf, epsilon=eps )\n        x_adv_tf = tf.convert_to_tensor(x_adv.numpy(), dtype=tf.float32)\n        y_pred_adv = (model.predict(x_adv_tf) > 0.5).astype(int).flatten()\n\n        adv_metrics = evaluate_metrics(y, y_pred_adv)\n        fgsm_results.append({'epsilon': eps, 'adversarial': adv_metrics})\n\n        print(f\"  FGSM ε={eps:.2f} → F1: {adv_metrics['f1']:.4f}, Precision: {adv_metrics['precision']:.4f}, Recall: {adv_metrics['recall']:.4f}, MCC: {adv_metrics.get('mcc', 'N/A')}, Specificity: {adv_metrics.get('specificity', 'N/A')}, Balanced Acc: {adv_metrics.get('balanced_accuracy', 'N/A')}\")\n\n    # ---------------- PGD Evaluation ----------------\n    print(\"\\nPGD Attack Evaluation:\")\n    pgd_results = []\n    for eps in epsilon_list:\n        alpha = eps * 0.2\n        x_adv = pgd_attack_tf(model, x_clean_tf, y_tf, epsilon=eps, alpha=alpha, num_iter=10)\n        x_adv_tf = tf.convert_to_tensor(x_adv.numpy(), dtype=tf.float32)\n        y_pred_adv = (model.predict(x_adv_tf) > 0.5).astype(int).flatten()\n\n        adv_metrics = evaluate_metrics(y, y_pred_adv)\n        pgd_results.append({'epsilon': eps, 'adversarial': adv_metrics})\n\n        print(f\"  PGD ε={eps:.2f} → F1: {adv_metrics['f1']:.4f}, Precision: {adv_metrics['precision']:.4f}, Recall: {adv_metrics['recall']:.4f}, MCC: {adv_metrics.get('mcc', 'N/A')}, Specificity: {adv_metrics.get('specificity', 'N/A')}, Balanced Acc: {adv_metrics.get('balanced_accuracy', 'N/A')}\")\n\n    # ---------------- MI-FGSM Evaluation ----------------\n    print(\"\\nMI-FGSM Attack Evaluation:\")\n    mifgsm_results = []\n    for eps in epsilon_list:\n        alpha = eps * 0.2\n        steps = 10   \n        decay = 1.0   \n\n        x_adv = mi_fgsm_attack(model, x_clean_tf, y_tf, epsilon=eps, alpha=alpha, steps=steps, decay=decay)\n        x_adv_tf = tf.convert_to_tensor(x_adv.numpy(), dtype=tf.float32)\n        y_pred_adv = (model.predict(x_adv_tf) > 0.5).astype(int).flatten()\n\n        adv_metrics = evaluate_metrics(y, y_pred_adv)\n        mifgsm_results.append({'epsilon': eps, 'adversarial': adv_metrics})\n\n        print(f\" MI-FGSM ε={eps:.2f} → F1: {adv_metrics['f1']:.4f}, Precision: {adv_metrics['precision']:.4f}, Recall: {adv_metrics['recall']:.4f}, MCC: {adv_metrics.get('mcc', 'N/A')}, Specificity: {adv_metrics.get('specificity', 'N/A')}, Balanced Acc: {adv_metrics.get('balanced_accuracy', 'N/A')}\")\n\n    return fgsm_results, pgd_results, mifgsm_results\n        \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T14:15:51.450981Z","iopub.execute_input":"2025-08-27T14:15:51.451207Z","iopub.status.idle":"2025-08-27T14:15:51.473296Z","shell.execute_reply.started":"2025-08-27T14:15:51.451188Z","shell.execute_reply":"2025-08-27T14:15:51.472572Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"print(\"CNN Results: \")\nepsilons = [.01,.03,.05,.07]\nfgsm_results, pgd_results, mifgsm_results = evaluate_model(\n    model=model,\n    X=X_test,\n    y=y_test,\n    epsilon_list=epsilons,\n)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T14:15:51.474112Z","iopub.execute_input":"2025-08-27T14:15:51.474360Z","iopub.status.idle":"2025-08-27T14:16:02.621571Z","shell.execute_reply.started":"2025-08-27T14:15:51.474337Z","shell.execute_reply":"2025-08-27T14:16:02.620983Z"}},"outputs":[{"name":"stdout","text":"CNN Results: \n✅ Clean Evaluation:\n\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n  F1: 0.9884, Precision: 0.9909, Recall: 0.9860, MCC: 0.9777571211186512, Specificity: 0.9915639195327387, Balanced Acc: 0.9887865068107905\n\nFGSM Attack Evaluation\n\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n  FGSM ε=0.01 → F1: 0.9467, Precision: 0.9529, Recall: 0.9405, MCC: 0.8978751208083884, Specificity: 0.9568462037637587, Balanced Acc: 0.9486924268206691\n\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n  FGSM ε=0.03 → F1: 0.6861, Precision: 0.6170, Recall: 0.7726, MCC: 0.3346969864578296, Specificity: 0.5551589876703259, Balanced Acc: 0.6639033833070063\n\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n  FGSM ε=0.05 → F1: 0.3708, Precision: 0.3095, Recall: 0.4624, MCC: -0.5495660703385877, Specificity: 0.04315379623620885, Balanced Acc: 0.2527766182999862\n\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n  FGSM ε=0.07 → F1: 0.3101, Precision: 0.2621, Recall: 0.3795, MCC: -0.6645566532954875, Specificity: 0.009085009733938706, Balanced Acc: 0.19429416628704632\n\nPGD Attack Evaluation:\n\u001b[1m111/186\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step","output_type":"stream"},{"name":"stderr","text":"2025-08-27 14:15:56.149741: E tensorflow/core/framework/node_def_util.cc:676] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_14}}\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n  PGD ε=0.01 → F1: 0.9380, Precision: 0.9447, Recall: 0.9314, MCC: 0.8813543610562042, Specificity: 0.9493835171965948, Balanced Acc: 0.9404140391159609\n\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n  PGD ε=0.03 → F1: 0.5586, Precision: 0.4830, Recall: 0.6621, MCC: 0.005013366937971592, Specificity: 0.3426346528228312, Balanced Acc: 0.5023771375341858\n\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n  PGD ε=0.05 → F1: 0.1705, Precision: 0.1534, Recall: 0.1920, MCC: -0.8076727701047912, Specificity: 0.016872160934457595, Balanced Acc: 0.10444867228254884\n\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n  PGD ε=0.07 → F1: 0.0998, Precision: 0.0919, Recall: 0.1091, MCC: -0.8994308778467633, Specificity: 0.0, Balanced Acc: 0.05456453305351522\n\nMI-FGSM Attack Evaluation:\n\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n MI-FGSM ε=0.01 → F1: 0.9384, Precision: 0.9447, Recall: 0.9321, MCC: 0.8820241587478624, Specificity: 0.9493835171965948, Balanced Acc: 0.9407638117637398\n\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n MI-FGSM ε=0.03 → F1: 0.5504, Precision: 0.4808, Recall: 0.6436, MCC: -0.0011788237758314384, Specificity: 0.3552887735236744, Balanced Acc: 0.4994352227184654\n\u001b[1m110/186\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step","output_type":"stream"},{"name":"stderr","text":"2025-08-27 14:16:01.391631: E tensorflow/core/framework/node_def_util.cc:676] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_14}}\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n MI-FGSM ε=0.05 → F1: 0.1663, Precision: 0.1498, Recall: 0.1868, MCC: -0.8121636148118978, Specificity: 0.016872160934457595, Balanced Acc: 0.10182537742420676\n\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n MI-FGSM ε=0.07 → F1: 0.0961, Precision: 0.0887, Recall: 0.1049, MCC: -0.9031452056708237, Specificity: 0.0, Balanced Acc: 0.05246589716684155\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"## Adversarial Training","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import Model\n\nclass PGDAdversarialTrainerCNN(tf.keras.Model):\n    def __init__(self, base_model, epsilon_range=(0.01, 0.1), alpha=0.01, num_iter=7, clip_min=-1.5, clip_max=1.5):\n        super().__init__()\n        self.base_model = base_model\n        self.epsilon_range = epsilon_range\n        self.alpha = alpha\n        self.num_iter = num_iter\n        self.clip_min = clip_min\n        self.clip_max = clip_max\n        self.loss_fn = tf.keras.losses.BinaryCrossentropy()\n        self.metric = tf.keras.metrics.BinaryAccuracy()\n\n    def compile(self, optimizer, **kwargs):\n        super().compile(**kwargs)\n        self.optimizer = optimizer\n\n    def pgd_attack_batch(self, x, y):\n        x_adv = tf.identity(x)\n        y = tf.expand_dims(tf.cast(y, tf.float32), axis=-1)\n        epsilon = tf.random.uniform([], *self.epsilon_range)\n\n        for _ in range(self.num_iter):\n            with tf.GradientTape() as tape:\n                tape.watch(x_adv)\n                preds = self.base_model(x_adv, training=True)\n                loss = self.loss_fn(y, preds)\n\n            grad = tape.gradient(loss, x_adv)\n            x_adv = x_adv + self.alpha * tf.sign(grad)\n            # Project back to epsilon ball around original input\n            x_adv = tf.clip_by_value(x_adv, x - epsilon, x + epsilon)\n            x_adv = tf.clip_by_value(x_adv, self.clip_min, self.clip_max)\n\n        return x_adv\n\n    def train_step(self, data):\n        x, y = data\n        x_adv = self.pgd_attack_batch(x, y)\n\n        # Combine clean and adversarial samples\n        x_combined = tf.concat([x, x_adv], axis=0)\n        y_combined = tf.concat([y, y], axis=0)\n        y_combined = tf.expand_dims(tf.cast(y_combined, tf.float32), axis=-1)\n\n        with tf.GradientTape() as tape:\n            preds = self.base_model(x_combined, training=True)\n            loss = self.loss_fn(y_combined, preds)\n\n        grads = tape.gradient(loss, self.base_model.trainable_variables)\n        self.optimizer.apply_gradients(zip(grads, self.base_model.trainable_variables))\n        self.metric.update_state(y_combined, preds)\n\n        return {\"loss\": loss, \"accuracy\": self.metric.result()}\n\n    def test_step(self, data):\n        x, y = data\n        y = tf.expand_dims(tf.cast(y, tf.float32), axis=-1)\n        preds = self.base_model(x, training=False)\n        loss = self.loss_fn(y, preds)\n        self.metric.update_state(y, preds)\n        return {\"loss\": loss, \"accuracy\": self.metric.result()}\n\n    def call(self, inputs):\n        return self.base_model(inputs)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T14:16:02.622404Z","iopub.execute_input":"2025-08-27T14:16:02.622720Z","iopub.status.idle":"2025-08-27T14:16:02.633917Z","shell.execute_reply.started":"2025-08-27T14:16:02.622687Z","shell.execute_reply":"2025-08-27T14:16:02.633162Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Wrap your existing CNN\n\nfrom tensorflow.keras.callbacks import EarlyStopping\n\nearly_stop = EarlyStopping(\n    monitor='val_loss',   # monitor validation loss\n    patience=3,           # stop if no improvement for 3 epochs\n    restore_best_weights=True\n)\n\n\nadv_model = PGDAdversarialTrainerCNN(\n    base_model=model,           \n    epsilon_range=(0.01, 0.05),\n    alpha=0.01,\n    num_iter=7,\n    clip_min=-1.5,\n    clip_max=1.5\n)\n\nadv_model.compile(optimizer=tf.keras.optimizers.Adam())\n\n# Train with adversarial training\nadv_model.fit(\n    x=X_train,\n    y=y_train,\n    validation_data=(X_val, y_val),\n    batch_size=256,\n    epochs=20,\n    callbacks=[early_stop] \n\n)\n\nprint(\"Advesarial training Results: \")\nepsilons = [.01,.03,.05,.07]\nfgsm_results, pgd_results, mifgsm_results = evaluate_model(\n    model=adv_model,\n    X=X_test,\n    y=y_test,\n    epsilon_list=epsilons,\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T14:16:02.634601Z","iopub.execute_input":"2025-08-27T14:16:02.634926Z","iopub.status.idle":"2025-08-27T14:18:32.458004Z","shell.execute_reply.started":"2025-08-27T14:16:02.634909Z","shell.execute_reply":"2025-08-27T14:18:32.457360Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/20\n\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8999 - loss: 0.2091","output_type":"stream"},{"name":"stderr","text":"2025-08-27 14:16:14.446431: E tensorflow/core/framework/node_def_util.cc:676] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_15}}\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.8999 - loss: 0.2088 - val_accuracy: 0.9777 - val_loss: 0.0574\nEpoch 2/20\n\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 14ms/step - accuracy: 0.9464 - loss: 0.1205 - val_accuracy: 0.9778 - val_loss: 0.0545\nEpoch 3/20\n\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 14ms/step - accuracy: 0.9557 - loss: 0.1041 - val_accuracy: 0.9772 - val_loss: 0.0517\nEpoch 4/20\n\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 14ms/step - accuracy: 0.9608 - loss: 0.0975 - val_accuracy: 0.9753 - val_loss: 0.0534\nEpoch 5/20\n\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 14ms/step - accuracy: 0.9625 - loss: 0.0905 - val_accuracy: 0.9773 - val_loss: 0.0496\nEpoch 6/20\n\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 14ms/step - accuracy: 0.9654 - loss: 0.0860 - val_accuracy: 0.9759 - val_loss: 0.0511\nEpoch 7/20\n\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 14ms/step - accuracy: 0.9658 - loss: 0.0830 - val_accuracy: 0.9766 - val_loss: 0.0485\nEpoch 8/20\n\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 14ms/step - accuracy: 0.9674 - loss: 0.0811 - val_accuracy: 0.9787 - val_loss: 0.0446\nEpoch 9/20\n\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 14ms/step - accuracy: 0.9692 - loss: 0.0783 - val_accuracy: 0.9792 - val_loss: 0.0438\nEpoch 10/20\n\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 14ms/step - accuracy: 0.9700 - loss: 0.0769 - val_accuracy: 0.9785 - val_loss: 0.0453\nEpoch 11/20\n\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 14ms/step - accuracy: 0.9703 - loss: 0.0745 - val_accuracy: 0.9800 - val_loss: 0.0424\nEpoch 12/20\n\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 14ms/step - accuracy: 0.9710 - loss: 0.0721 - val_accuracy: 0.9777 - val_loss: 0.0442\nEpoch 13/20\n\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 15ms/step - accuracy: 0.9713 - loss: 0.0708 - val_accuracy: 0.9803 - val_loss: 0.0393\nEpoch 14/20\n\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 14ms/step - accuracy: 0.9724 - loss: 0.0711 - val_accuracy: 0.9792 - val_loss: 0.0432\nEpoch 15/20\n\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 14ms/step - accuracy: 0.9717 - loss: 0.0698 - val_accuracy: 0.9797 - val_loss: 0.0415\nEpoch 16/20\n\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 14ms/step - accuracy: 0.9729 - loss: 0.0674 - val_accuracy: 0.9810 - val_loss: 0.0374\nEpoch 17/20\n\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 15ms/step - accuracy: 0.9732 - loss: 0.0669 - val_accuracy: 0.9805 - val_loss: 0.0384\nEpoch 18/20\n\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 14ms/step - accuracy: 0.9728 - loss: 0.0664 - val_accuracy: 0.9807 - val_loss: 0.0370\nEpoch 19/20\n\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 14ms/step - accuracy: 0.9734 - loss: 0.0644 - val_accuracy: 0.9846 - val_loss: 0.0319\nEpoch 20/20\n\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 15ms/step - accuracy: 0.9736 - loss: 0.0643 - val_accuracy: 0.9810 - val_loss: 0.0363\nAdvesarial training Results: \n✅ Clean Evaluation:\n\u001b[1m 71/186\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step","output_type":"stream"},{"name":"stderr","text":"2025-08-27 14:18:22.119714: E tensorflow/core/framework/node_def_util.cc:676] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_14}}\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n  F1: 0.9865, Precision: 0.9912, Recall: 0.9818, MCC: 0.9740733722996718, Specificity: 0.9918883841660937, Balanced Acc: 0.9868501032407944\n\nFGSM Attack Evaluation\n\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n  FGSM ε=0.01 → F1: 0.9790, Precision: 0.9865, Recall: 0.9717, MCC: 0.9599620181823939, Specificity: 0.9876703439324793, Balanced Acc: 0.9796693797311924\n\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n  FGSM ε=0.03 → F1: 0.9690, Precision: 0.9820, Recall: 0.9563, MCC: 0.9412496738371114, Specificity: 0.9837767683322198, Balanced Acc: 0.9700275936799259\n\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n  FGSM ε=0.05 → F1: 0.9554, Precision: 0.9708, Recall: 0.9405, MCC: 0.9157010041752187, Specificity: 0.9737183646982163, Balanced Acc: 0.9571285072878979\n\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n  FGSM ε=0.07 → F1: 0.9260, Precision: 0.9418, Recall: 0.9108, MCC: 0.8600245905060541, Specificity: 0.94776119402982, Balanced Acc: 0.9292845844230946\n\nPGD Attack Evaluation:\n\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n  PGD ε=0.01 → F1: 0.9769, Precision: 0.9844, Recall: 0.9696, MCC: 0.9559139662836434, Specificity: 0.9857235561323495, Balanced Acc: 0.9776466678877906\n\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n  PGD ε=0.03 → F1: 0.9628, Precision: 0.9746, Recall: 0.9514, MCC: 0.9293887718287918, Specificity: 0.9769630110317659, Balanced Acc: 0.9641723064952463\n\u001b[1m101/186\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step","output_type":"stream"},{"name":"stderr","text":"2025-08-27 14:18:27.715004: E tensorflow/core/framework/node_def_util.cc:676] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_14}}\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n  PGD ε=0.05 → F1: 0.9369, Precision: 0.9500, Recall: 0.9241, MCC: 0.8801721390461944, Specificity: 0.9548994159636289, Balanced Acc: 0.9394993756977991\n\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n  PGD ε=0.07 → F1: 0.8790, Precision: 0.8851, Recall: 0.8730, MCC: 0.7683516183297019, Specificity: 0.8948734587929625, Balanced Acc: 0.883952993824603\n\nMI-FGSM Attack Evaluation:\n\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n MI-FGSM ε=0.01 → F1: 0.9769, Precision: 0.9844, Recall: 0.9696, MCC: 0.9559139662836434, Specificity: 0.9857235561323495, Balanced Acc: 0.9776466678877906\n\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n MI-FGSM ε=0.03 → F1: 0.9630, Precision: 0.9749, Recall: 0.9514, MCC: 0.929732636920148, Specificity: 0.9772874756651208, Balanced Acc: 0.9643345388119238\n\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n MI-FGSM ε=0.05 → F1: 0.9380, Precision: 0.9501, Recall: 0.9262, MCC: 0.8821612550726982, Specificity: 0.9548994159636289, Balanced Acc: 0.9405486936411359\n\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n MI-FGSM ε=0.07 → F1: 0.8782, Precision: 0.8816, Recall: 0.8748, MCC: 0.7659955927640413, Specificity: 0.890979883192703, Balanced Acc: 0.8828806376439207\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# generate adversarial values for gan training\n\n# experimental PGD code (GPU-optimized, batched)\ndef pgd_attack_tf_fast(model, x, y, epsilon=0.05, alpha=0.01, num_iter=10, clip_min=-1.5, clip_max=1.5):\n    x = tf.convert_to_tensor(x, dtype=tf.float32)\n    y = tf.reshape(tf.cast(y, tf.float32), (-1, 1))\n    x_orig = tf.identity(x)\n    x_adv = tf.identity(x)\n\n    for _ in range(num_iter):\n        with tf.GradientTape() as tape:\n            tape.watch(x_adv)\n            logits = model(x_adv, training=False)\n            loss = tf.keras.losses.binary_crossentropy(y, logits)\n            loss = tf.reduce_mean(loss)\n\n        grad = tape.gradient(loss, x_adv)\n        signed_grad = tf.sign(grad)\n        x_adv = x_adv + alpha * signed_grad\n        x_adv = tf.clip_by_value(x_adv, x_orig - epsilon, x_orig + epsilon)\n        x_adv = tf.clip_by_value(x_adv, clip_min, clip_max)\n\n    return x_adv\n\ndef batch_pgd_attack(model, X, y, attack_fn, batch_size=1024, **kwargs):\n    adv_list = []\n    X = tf.convert_to_tensor(X, dtype=tf.float32)\n    y = tf.convert_to_tensor(y, dtype=tf.float32)\n    for i in range(0, len(X), batch_size):\n        X_batch = X[i:i+batch_size]\n        y_batch = y[i:i+batch_size]\n        x_adv_batch = attack_fn(model, X_batch, y_batch, **kwargs)\n        adv_list.append(x_adv_batch)\n    return tf.concat(adv_list, axis=0)\n\n# Generate adversarial examples in batches to leverage GPU\nX_adv_train = batch_pgd_attack(model, X_train, y_train, pgd_attack_tf_fast, epsilon=0.05, batch_size=1024)\nprint(f\"X_adv_train shape: {X_adv_train.shape}\")\n\nX_adv_val = batch_pgd_attack(model, X_val, y_val, pgd_attack_tf_fast, epsilon=0.05, batch_size=1024)\nprint(f\"X_adv_val shape: {X_adv_val.shape}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T14:18:32.458790Z","iopub.execute_input":"2025-08-27T14:18:32.459261Z","iopub.status.idle":"2025-08-27T14:18:58.728542Z","shell.execute_reply.started":"2025-08-27T14:18:32.459241Z","shell.execute_reply":"2025-08-27T14:18:58.727932Z"}},"outputs":[{"name":"stdout","text":"X_adv_train shape: (118813, 84, 1)\nX_adv_val shape: (23763, 84, 1)\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, Model\nfrom tensorflow.keras.callbacks import EarlyStopping\n\n# ------------------------------\n# Generator CNN (1D output)\n# ------------------------------\nclass GeneratorCNN(Model):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = layers.Conv1D(64, 3, padding='same', activation='relu')\n        self.bn1 = layers.BatchNormalization()\n        self.conv2 = layers.Conv1D(128, 3, padding='same', activation='relu')\n        self.bn2 = layers.BatchNormalization()\n        self.conv3 = layers.Conv1D(64, 3, padding='same', activation='relu')\n        self.bn3 = layers.BatchNormalization()\n        # final output: 1 channel (same as clean input)\n        self.output_layer = layers.Conv1D(1, 3, padding='same', activation='linear')\n\n    def call(self, x, training=False):\n        x = self.conv1(x)\n        x = self.bn1(x, training=training)\n        x = self.conv2(x)\n        x = self.bn2(x, training=training)\n        x = self.conv3(x)\n        x = self.bn3(x, training=training)\n        return self.output_layer(x)\n\n\n# ------------------------------\n# Discriminator CNN (1D input)\n# ------------------------------\nclass DiscriminatorCNN(Model):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = layers.Conv1D(64, 3, padding='same', activation='relu')\n        self.global_pool = layers.GlobalAveragePooling1D()\n        self.dense1 = layers.Dense(64, activation='relu')\n        self.output_layer = layers.Dense(1, activation='sigmoid')\n\n    def call(self, x, training=False):\n        x = self.conv1(x)\n        x = self.global_pool(x)\n        x = self.dense1(x)\n        return self.output_layer(x)\n\n\n# ------------------------------\n# GAN Purifier CNN\n# ------------------------------\nclass GANPurifierCNN(Model):\n    def __init__(self, generator, discriminator, lambda_adv=0.5):\n        super().__init__()\n        self.generator = generator\n        self.discriminator = discriminator\n        self.lambda_adv = lambda_adv\n        self.bce = tf.keras.losses.BinaryCrossentropy()\n        self.l1 = tf.keras.losses.MeanAbsoluteError()\n        self.d_optimizer = tf.keras.optimizers.Adam(1e-4)\n        self.g_optimizer = tf.keras.optimizers.Adam(1e-4)\n\n    def compile(self, **kwargs):\n        super().compile(**kwargs)\n\n    def train_step(self, data):\n        x_clean, x_adv = data\n        with tf.GradientTape(persistent=True) as tape:\n            x_fake = self.generator(x_adv, training=True)\n            real_pred = self.discriminator(x_clean, training=True)\n            fake_pred = self.discriminator(x_fake, training=True)\n\n            # Discriminator loss\n            d_loss = self.bce(tf.ones_like(real_pred), real_pred) + \\\n                     self.bce(tf.zeros_like(fake_pred), fake_pred)\n\n            # Generator loss (reconstruction + adversarial)\n            g_loss = self.l1(x_clean, x_fake) + \\\n                     self.lambda_adv * self.bce(tf.ones_like(fake_pred), fake_pred)\n\n            # Cosine similarity\n            x_clean_flat = tf.reshape(x_clean, (tf.shape(x_clean)[0], -1))\n            x_fake_flat = tf.reshape(x_fake, (tf.shape(x_fake)[0], -1))\n            cos_sim = -tf.reduce_mean(\n                tf.keras.losses.cosine_similarity(x_clean_flat, x_fake_flat)\n            )\n\n        # Compute gradients\n        d_grads = tape.gradient(d_loss, self.discriminator.trainable_variables)\n        g_grads = tape.gradient(g_loss, self.generator.trainable_variables)\n\n        # Apply gradients\n        self.d_optimizer.apply_gradients(zip(d_grads, self.discriminator.trainable_variables))\n        self.g_optimizer.apply_gradients(zip(g_grads, self.generator.trainable_variables))\n\n        return {\"d_loss\": d_loss, \"g_loss\": g_loss,\n                \"reconstruction_loss\": self.l1(x_clean, x_fake),\n                \"cosine_similarity\": cos_sim}\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T14:18:58.729384Z","iopub.execute_input":"2025-08-27T14:18:58.729688Z","iopub.status.idle":"2025-08-27T14:18:58.743372Z","shell.execute_reply.started":"2025-08-27T14:18:58.729647Z","shell.execute_reply":"2025-08-27T14:18:58.742647Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# ------------------------------\n# Instantiate models\n# ------------------------------\ngenerator = GeneratorCNN()\ndiscriminator = DiscriminatorCNN()\n\n# Build models with dummy input (1 channel)\ndummy_input = tf.zeros((1, 95, 1))\ngenerator(dummy_input)\ndiscriminator(dummy_input)\n\ngan_cnn = GANPurifierCNN(generator, discriminator, lambda_adv=0.5)\ngan_cnn.compile()\n\n# ------------------------------\n# Prepare dataset\n# ------------------------------\nbatch_size = 512\ntrain_dataset = tf.data.Dataset.from_tensor_slices((X_train, X_adv_train))\ntrain_dataset = train_dataset.shuffle(10000).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n\nearly_stop = EarlyStopping(\n    monitor=\"cosine_similarity\",\n    mode=\"max\",              # higher cosine similarity is better\n    patience=10,\n    min_delta=1e-4,\n    restore_best_weights=True,\n    verbose=1\n)\n\n# ------------------------------\n# Train\n# ------------------------------\ngan_cnn.fit(train_dataset, epochs=100, callbacks=[early_stop])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T14:18:58.744146Z","iopub.execute_input":"2025-08-27T14:18:58.744498Z","iopub.status.idle":"2025-08-27T14:21:04.342868Z","shell.execute_reply.started":"2025-08-27T14:18:58.744473Z","shell.execute_reply":"2025-08-27T14:21:04.342225Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/100\n\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 24ms/step - cosine_similarity: 0.7854 - d_loss: 1.3749 - g_loss: 0.5369 - reconstruction_loss: 0.1876\nEpoch 2/100\n\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 24ms/step - cosine_similarity: 0.9733 - d_loss: 1.3810 - g_loss: 0.3920 - reconstruction_loss: 0.0440\nEpoch 3/100\n\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 24ms/step - cosine_similarity: 0.9845 - d_loss: 1.3813 - g_loss: 0.3802 - reconstruction_loss: 0.0323\nEpoch 4/100\n\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 24ms/step - cosine_similarity: 0.9886 - d_loss: 1.3809 - g_loss: 0.3752 - reconstruction_loss: 0.0272\nEpoch 5/100\n\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 24ms/step - cosine_similarity: 0.9909 - d_loss: 1.3804 - g_loss: 0.3721 - reconstruction_loss: 0.0240\nEpoch 6/100\n\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 24ms/step - cosine_similarity: 0.9923 - d_loss: 1.3797 - g_loss: 0.3699 - reconstruction_loss: 0.0216\nEpoch 7/100\n\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 24ms/step - cosine_similarity: 0.9930 - d_loss: 1.3786 - g_loss: 0.3693 - reconstruction_loss: 0.0207\nEpoch 8/100\n\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 24ms/step - cosine_similarity: 0.9935 - d_loss: 1.3773 - g_loss: 0.3689 - reconstruction_loss: 0.0200\nEpoch 9/100\n\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 24ms/step - cosine_similarity: 0.9940 - d_loss: 1.3753 - g_loss: 0.3685 - reconstruction_loss: 0.0192\nEpoch 10/100\n\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 24ms/step - cosine_similarity: 0.9942 - d_loss: 1.3721 - g_loss: 0.3688 - reconstruction_loss: 0.0187\nEpoch 11/100\n\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 24ms/step - cosine_similarity: 0.9946 - d_loss: 1.3669 - g_loss: 0.3691 - reconstruction_loss: 0.0178\nEpoch 12/100\n\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 24ms/step - cosine_similarity: 0.9947 - d_loss: 1.3570 - g_loss: 0.3721 - reconstruction_loss: 0.0182\nEpoch 13/100\n\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 24ms/step - cosine_similarity: 0.9950 - d_loss: 1.3501 - g_loss: 0.3726 - reconstruction_loss: 0.0169\nEpoch 14/100\n\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 24ms/step - cosine_similarity: 0.9951 - d_loss: 1.3367 - g_loss: 0.3763 - reconstruction_loss: 0.0168\nEpoch 15/100\n\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 24ms/step - cosine_similarity: 0.9949 - d_loss: 1.3162 - g_loss: 0.3827 - reconstruction_loss: 0.0176\nEpoch 16/100\n\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 24ms/step - cosine_similarity: 0.9947 - d_loss: 1.2982 - g_loss: 0.3875 - reconstruction_loss: 0.0179\nEpoch 17/100\n\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 24ms/step - cosine_similarity: 0.9950 - d_loss: 1.2970 - g_loss: 0.3861 - reconstruction_loss: 0.0161\nEpoch 18/100\n\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 24ms/step - cosine_similarity: 0.9947 - d_loss: 1.2698 - g_loss: 0.3951 - reconstruction_loss: 0.0171\nEpoch 19/100\n\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 24ms/step - cosine_similarity: 0.9944 - d_loss: 1.2439 - g_loss: 0.4038 - reconstruction_loss: 0.0180\nEpoch 20/100\n\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 24ms/step - cosine_similarity: 0.9947 - d_loss: 1.2415 - g_loss: 0.4030 - reconstruction_loss: 0.0166\nEpoch 21/100\n\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 24ms/step - cosine_similarity: 0.9941 - d_loss: 1.2135 - g_loss: 0.4129 - reconstruction_loss: 0.0180\nEpoch 21: early stopping\nRestoring model weights from the end of the best epoch: 11.\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.history.History at 0x7b5ef52f6c90>"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"# Generate purified outputs\nx_purified_val = generator(X_adv_val, training=False)\n\n# Cast to float32 for evaluation\nx_purified_val = tf.cast(x_purified_val, tf.float32)\nX_val_cast = tf.cast(X_val, tf.float32)\n\n# L1 (Mean Absolute Error)\nl1_loss = tf.reduce_mean(tf.abs(X_val_cast - x_purified_val))\n\n# L2 (Mean Squared Error)\nl2_loss = tf.reduce_mean(tf.square(X_val_cast - x_purified_val))\n\n# Normalize along sequence dimension\nX_val_norm = tf.nn.l2_normalize(tf.squeeze(X_val_cast, axis=-1), axis=1)           \nx_purified_norm = tf.nn.l2_normalize(tf.squeeze(x_purified_val, axis=-1), axis=1)  \n\n# Cosine similarity\ncos_sim = tf.reduce_mean(tf.reduce_sum(X_val_norm * x_purified_norm, axis=1))\n\nprint(f\"🧪 Purifier Evaluation Metrics:\")\nprint(f\"✅ Cosine Similarity: {cos_sim:.6f}\")\nprint(f\"   ➤ L1 Loss (MAE): {l1_loss:.6f}\")\nprint(f\"   ➤ L2 Loss (MSE): {l2_loss:.6f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T14:21:04.343583Z","iopub.execute_input":"2025-08-27T14:21:04.344427Z","iopub.status.idle":"2025-08-27T14:21:04.767758Z","shell.execute_reply.started":"2025-08-27T14:21:04.344397Z","shell.execute_reply":"2025-08-27T14:21:04.767119Z"}},"outputs":[{"name":"stdout","text":"🧪 Purifier Evaluation Metrics:\n✅ Cosine Similarity: 0.991526\n   ➤ L1 Loss (MAE): 0.027191\n   ➤ L2 Loss (MSE): 0.001684\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"## GAN+Adv Training evaluation","metadata":{}},{"cell_type":"code","source":"def evaluate_model(model, generator, adv_model, X, y, epsilon_list=[0.01, 0.03, 0.05, 0.07]):\n    import tensorflow as tf\n    import numpy as np\n\n    X_clean = X\n    x_clean_tf = tf.convert_to_tensor(X_clean, dtype=tf.float32)\n    y_tf = tf.convert_to_tensor(y, dtype=tf.float32)\n\n    # ----------- Clean Data Evaluation (no adversarial) -----------\n    print(\"✅ Clean Evaluation (GAN purified):\")\n    x_purified_clean = generator(x_clean_tf, training=False)\n    y_pred_clean_prob = adv_model.predict(x_purified_clean)\n    y_pred_clean = (y_pred_clean_prob > 0.5).astype(int).flatten()\n    clean_metrics = evaluate_metrics(y, y_pred_clean)\n    print(f\"Clean → F1: {clean_metrics['f1']:.4f}, Precision: {clean_metrics['precision']:.4f}, Recall: {clean_metrics['recall']:.4f}, MCC: {clean_metrics.get('mcc', 'N/A')}, Specificity: {clean_metrics.get('specificity', 'N/A')}, Balanced Acc: {clean_metrics.get('balanced_accuracy', 'N/A')}\")\n\n    # ----------- FGSM Attack Evaluation -----------\n    print(\"\\nFGSM Attack Evaluation with GAN Purification\")\n    fgsm_results = []\n    for eps in epsilon_list:\n        x_adv = fgsm_attack_tf(model, x_clean_tf, y_tf, epsilon=eps )\n        x_adv_tf = tf.convert_to_tensor(x_adv.numpy(), dtype=tf.float32)\n        x_purified = generator(x_adv_tf, training=False)\n        y_pred_prob = adv_model.predict(x_purified)\n        y_pred = (y_pred_prob > 0.5).astype(int).flatten()\n\n        adv_metrics = evaluate_metrics(y, y_pred)\n        fgsm_results.append({'epsilon': eps, 'adversarial': adv_metrics})\n\n        print(f\"  FGSM ε={eps:.2f} → F1: {adv_metrics['f1']:.4f}, Precision: {adv_metrics['precision']:.4f}, Recall: {adv_metrics['recall']:.4f}, MCC: {adv_metrics.get('mcc', 'N/A')}, Specificity: {adv_metrics.get('specificity', 'N/A')}, Balanced Acc: {adv_metrics.get('balanced_accuracy', 'N/A')}\")\n\n    # ----------- PGD Attack Evaluation -----------\n    print(\"\\nPGD Attack Evaluation with GAN Purification:\")\n    pgd_results = []\n    for eps in epsilon_list:\n        alpha = eps * 0.2\n        x_adv = pgd_attack_tf(model, x_clean_tf, y_tf, epsilon=eps, alpha=alpha, num_iter=10)\n        x_adv_tf = tf.convert_to_tensor(x_adv.numpy(), dtype=tf.float32)\n        x_purified = generator(x_adv_tf, training=False)\n        y_pred_prob = adv_model.predict(x_purified )\n        y_pred = (y_pred_prob > 0.5).astype(int).flatten()\n\n        adv_metrics = evaluate_metrics(y, y_pred)\n        pgd_results.append({'epsilon': eps, 'adversarial': adv_metrics})\n\n        print(f\"  PGD ε={eps:.2f} → F1: {adv_metrics['f1']:.4f}, Precision: {adv_metrics['precision']:.4f}, Recall: {adv_metrics['recall']:.4f}, MCC: {adv_metrics.get('mcc', 'N/A')}, Specificity: {adv_metrics.get('specificity', 'N/A')}, Balanced Acc: {adv_metrics.get('balanced_accuracy', 'N/A')}\")\n\n      # ----------- MI-FGSM Attack Evaluation -----------\n    print(\"\\nMI-FGSM Attack Evaluation with GAN Purification:\")\n    mifgsm_results = []\n    for eps in epsilon_list:\n        alpha = eps * 0.2\n        steps = 10\n        decay = 1.0\n\n        x_adv = mi_fgsm_attack(model, x_clean_tf, y_tf, epsilon=eps, alpha=alpha, steps=steps, decay=decay)\n        x_adv_tf = tf.convert_to_tensor(x_adv.numpy(), dtype=tf.float32)\n        x_purified = generator(x_adv_tf, training=False)\n        y_pred_prob = adv_model.predict(x_purified )\n        y_pred = (y_pred_prob > 0.5).astype(int).flatten()\n\n        adv_metrics = evaluate_metrics(y, y_pred)\n        mifgsm_results.append({'epsilon': eps, 'adversarial': adv_metrics})\n\n        print(f\"  MI-FGSM ε={eps:.2f} → F1: {adv_metrics['f1']:.4f}, Precision: {adv_metrics['precision']:.4f}, Recall: {adv_metrics['recall']:.4f}, MCC: {adv_metrics.get('mcc', 'N/A')}, Specificity: {adv_metrics.get('specificity', 'N/A')}, Balanced Acc: {adv_metrics.get('balanced_accuracy', 'N/A')}\")\n\n    return fgsm_results, pgd_results, mifgsm_results\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T14:21:04.768499Z","iopub.execute_input":"2025-08-27T14:21:04.768775Z","iopub.status.idle":"2025-08-27T14:21:04.779340Z","shell.execute_reply.started":"2025-08-27T14:21:04.768747Z","shell.execute_reply":"2025-08-27T14:21:04.778584Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"fgsm_full, pgd_full, mi_full = evaluate_model(model= model,generator=generator,adv_model= model, X = X_test, y = y_test)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T14:21:04.780104Z","iopub.execute_input":"2025-08-27T14:21:04.780293Z","iopub.status.idle":"2025-08-27T14:21:12.041502Z","shell.execute_reply.started":"2025-08-27T14:21:04.780279Z","shell.execute_reply":"2025-08-27T14:21:12.040862Z"}},"outputs":[{"name":"stdout","text":"✅ Clean Evaluation (GAN purified):\n\u001b[1m110/186\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step","output_type":"stream"},{"name":"stderr","text":"2025-08-27 14:21:04.906456: E tensorflow/core/framework/node_def_util.cc:676] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_14}}\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\nClean → F1: 0.9730, Precision: 0.9895, Recall: 0.9570, MCC: 0.949174780694604, Specificity: 0.9905905256326739, Balanced Acc: 0.9737842449779319\n\nFGSM Attack Evaluation with GAN Purification\n\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n  FGSM ε=0.01 → F1: 0.9701, Precision: 0.9884, Recall: 0.9524, MCC: 0.9438712617455394, Specificity: 0.989617131732609, Balanced Acc: 0.9710240258173364\n\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n  FGSM ε=0.03 → F1: 0.9648, Precision: 0.9851, Recall: 0.9454, MCC: 0.9341952075451513, Specificity: 0.9866969500324144, Balanced Acc: 0.9660662084894496\n\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n  FGSM ε=0.05 → F1: 0.9495, Precision: 0.9774, Recall: 0.9231, MCC: 0.9063869785192996, Specificity: 0.9802076573653153, Balanced Acc: 0.9516288374269739\n\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n  FGSM ε=0.07 → F1: 0.9399, Precision: 0.9612, Recall: 0.9196, MCC: 0.8873281910189686, Specificity: 0.9656067488643424, Balanced Acc: 0.9425795199375926\n\nPGD Attack Evaluation with GAN Purification:\n\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n  PGD ε=0.01 → F1: 0.9701, Precision: 0.9884, Recall: 0.9524, MCC: 0.9438712617455394, Specificity: 0.989617131732609, Balanced Acc: 0.9710240258173364\n\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n  PGD ε=0.03 → F1: 0.9646, Precision: 0.9861, Recall: 0.9440, MCC: 0.93393998740253, Specificity: 0.9876703439324793, Balanced Acc: 0.9658533601439241\n\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n  PGD ε=0.05 → F1: 0.9520, Precision: 0.9789, Recall: 0.9265, MCC: 0.9110320970346104, Specificity: 0.9815055158987351, Balanced Acc: 0.9540266299325785\n\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n  PGD ε=0.07 → F1: 0.9365, Precision: 0.9641, Recall: 0.9105, MCC: 0.882030345050971, Specificity: 0.968526930564537, Balanced Acc: 0.9394925663665636\n\nMI-FGSM Attack Evaluation with GAN Purification:\n\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n  MI-FGSM ε=0.01 → F1: 0.9703, Precision: 0.9884, Recall: 0.9528, MCC: 0.944198583809832, Specificity: 0.989617131732609, Balanced Acc: 0.9711989121412259\n\u001b[1m111/186\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step","output_type":"stream"},{"name":"stderr","text":"2025-08-27 14:21:10.433322: E tensorflow/core/framework/node_def_util.cc:676] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_14}}\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n  MI-FGSM ε=0.03 → F1: 0.9639, Precision: 0.9850, Recall: 0.9437, MCC: 0.9325642008008482, Specificity: 0.9866969500324144, Balanced Acc: 0.9651917768700022\n\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n  MI-FGSM ε=0.05 → F1: 0.9471, Precision: 0.9755, Recall: 0.9203, MCC: 0.9020352779180763, Specificity: 0.9785853341985405, Balanced Acc: 0.9494185852524707\n\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n  MI-FGSM ε=0.07 → F1: 0.9331, Precision: 0.9546, Recall: 0.9126, MCC: 0.874508094922689, Specificity: 0.9597663854639533, Balanced Acc: 0.9361616117596088\n","output_type":"stream"}],"execution_count":17}]}