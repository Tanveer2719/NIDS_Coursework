{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":394223,"sourceType":"datasetVersion","datasetId":174616}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/Tanveer2719/NIDS_Coursework.git\n%cd /kaggle/working/NIDS_Coursework/My_Code\n!ls /kaggle/input\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-22T16:40:19.079089Z","iopub.execute_input":"2025-09-22T16:40:19.079402Z","iopub.status.idle":"2025-09-22T16:40:20.127370Z","shell.execute_reply.started":"2025-09-22T16:40:19.079376Z","shell.execute_reply":"2025-09-22T16:40:20.126562Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'NIDS_Coursework'...\nremote: Enumerating objects: 262, done.\u001b[K\nremote: Counting objects: 100% (262/262), done.\u001b[K\nremote: Compressing objects: 100% (171/171), done.\u001b[K\nremote: Total 262 (delta 126), reused 221 (delta 85), pack-reused 0 (from 0)\u001b[K\nReceiving objects: 100% (262/262), 1.13 MiB | 8.20 MiB/s, done.\nResolving deltas: 100% (126/126), done.\n/kaggle/working/NIDS_Coursework/My_Code\nnslkdd\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport random\nimport numpy as np\nimport tensorflow as tf\n\ndef set_seed(seed=42):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    tf.keras.utils.set_random_seed(seed)\n    tf.config.experimental.enable_op_determinism()\n\nset_seed(42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T12:54:42.796477Z","iopub.execute_input":"2025-08-27T12:54:42.796731Z","iopub.status.idle":"2025-08-27T12:54:56.920208Z","shell.execute_reply.started":"2025-08-27T12:54:42.796695Z","shell.execute_reply":"2025-08-27T12:54:56.919640Z"}},"outputs":[{"name":"stderr","text":"2025-08-27 12:54:44.414676: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1756299284.616094      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1756299284.673231      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nos.listdir(\"/kaggle/input\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-22T16:40:40.831778Z","iopub.execute_input":"2025-09-22T16:40:40.832114Z","iopub.status.idle":"2025-09-22T16:40:40.839366Z","shell.execute_reply.started":"2025-09-22T16:40:40.832084Z","shell.execute_reply":"2025-09-22T16:40:40.838682Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"['nslkdd']"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nurl_train = \"/kaggle/input/nslkdd/KDDTrain+.txt\"\nurl_test = \"/kaggle/input/nslkdd/KDDTest+.txt\"\ncolumns = [\n    'duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes',\n    'land', 'wrong_fragment', 'urgent', 'hot', 'num_failed_logins',\n    'logged_in', 'num_compromised', 'root_shell', 'su_attempted', 'num_root',\n    'num_file_creations', 'num_shells', 'num_access_files', 'num_outbound_cmds',\n    'is_host_login', 'is_guest_login', 'count', 'srv_count', 'serror_rate',\n    'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate', 'same_srv_rate',\n    'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count',\n    'dst_host_srv_count', 'dst_host_same_srv_rate', 'dst_host_diff_srv_rate',\n    'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate',\n    'dst_host_serror_rate', 'dst_host_srv_serror_rate', 'dst_host_rerror_rate',\n    'dst_host_srv_rerror_rate', 'attack', 'level'\n]\n\ntrain_df = pd.read_csv(url_train, names=columns)\ntest_df = pd.read_csv(url_test, names=columns)\n\n\n# Drop the \"type\" column (not needed)\ntrain_df.drop(columns=['level'], inplace=True)\ntest_df.drop(columns=['level'], inplace=True)\n\n# Strip whitespace from class labels\nfor df in [train_df, test_df]:\n    df['attack'] = df['attack'].str.strip()\n\n\n\n# Combine train and test\nfull_df = pd.concat([train_df, test_df], ignore_index=True)\n\nfull_df['label'] = full_df['attack'].apply(lambda x: 0 if x == 'normal' else 1)\nfull_df = full_df.drop(columns=['attack'])\n\nprint(f'full_df.shape : {full_df.shape}')\n\n\n\n\n# --------------------------- perform preprocessing--------------\n\nfrom preprocess import Preprocess\n\n# separate the features and labels so that the labesl are not encoded\nlabels = full_df['label']\nfeatures = full_df.drop(columns=['label'])\n\npp = Preprocess()\nprocessed_features = pp.fit_transform_df_auto(df = features,n_categorical_levels=32, expected_categorical_format='onehot')\n\nprint(processed_features.shape)\n\n# concat the features and the labels\nprocessed_full_df = pd.concat([processed_features, labels], axis=1)\n\n# Filter out categorical one-hot encoded columns\nnumerical_columns = [col for col in processed_full_df \n                     if not (col.startswith(\"proto_\") or \n                             col.startswith(\"service_\") or \n                             col.startswith(\"state_\") or\n                             col == 'label')]\n\ntarget = processed_full_df['label']\nfeatures = processed_full_df.drop(columns=['label'])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-22T16:40:46.663562Z","iopub.execute_input":"2025-09-22T16:40:46.664310Z","iopub.status.idle":"2025-09-22T16:40:49.046730Z","shell.execute_reply.started":"2025-09-22T16:40:46.664284Z","shell.execute_reply":"2025-09-22T16:40:49.045956Z"}},"outputs":[{"name":"stdout","text":"full_df.shape : (148517, 42)\nEncoding the 3 levels for protocol_type\nEncoding the 32 levels for service\nEncoding the 11 levels for flag\n(148517, 84)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import tensorflow as tf\nfrom sklearn.model_selection import train_test_split\n\nX = features.astype(\"float32\").values  # (257673, 95) as float32\ny = target.astype(\"int32\").values      # (257673,)\n\n\n# reshape for (samples, timesteps, features=1)\nX = np.expand_dims(X, axis=-1)  # (257673, 95, 1)\n\n# Split train, val, test\nX_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\nX_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.2, random_state=42, stratify=y_temp)\n\nprint(f\"X_train: {X_train.shape}\")\nprint(f\"y_train: {y_train.shape}\")\nprint(f\"X_val: {X_val.shape}\")\nprint(f\"y_val: {y_val.shape}\")\nprint(f\"X_test: {X_test.shape}\")\nprint(f\"y_test: {y_test.shape}\")\n\n\n\ninput_shape = (X_train.shape[1], X_train.shape[2])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-22T16:40:57.166634Z","iopub.execute_input":"2025-09-22T16:40:57.167089Z","iopub.status.idle":"2025-09-22T16:41:10.919261Z","shell.execute_reply.started":"2025-09-22T16:40:57.167062Z","shell.execute_reply":"2025-09-22T16:41:10.918591Z"}},"outputs":[{"name":"stderr","text":"2025-09-22 16:40:58.787936: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1758559259.004615      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1758559259.068028      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"X_train: (118813, 84, 1)\ny_train: (118813,)\nX_val: (23763, 84, 1)\ny_val: (23763,)\nX_test: (5941, 84, 1)\ny_test: (5941,)\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n\ndef evaluate_metrics(y_true, y_pred):\n    f1 = f1_score(y_true, y_pred)\n    precision = precision_score(y_true, y_pred)\n    recall = recall_score(y_true, y_pred)\n    cm = confusion_matrix(y_true, y_pred)\n\n    if cm.shape == (2, 2):\n        tn, fp, fn, tp = cm.ravel()\n        numerator = tp * tn - fp * fn\n        denominator = ((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)) ** 0.5\n        mcc = numerator / (denominator + 1e-10)\n        specificity = tn / (tn + fp + 1e-10)\n        balanced_acc = (recall + specificity) / 2\n    else:\n        mcc = 0\n        balanced_acc = 0\n        specificity = 0\n\n    return {\n        'f1': f1,\n        'precision': precision,\n        'recall': recall,\n        'mcc': mcc,\n        'specificity': specificity,\n        'balanced_accuracy': balanced_acc\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-22T16:41:10.920606Z","iopub.execute_input":"2025-09-22T16:41:10.921088Z","iopub.status.idle":"2025-09-22T16:41:10.927309Z","shell.execute_reply.started":"2025-09-22T16:41:10.921068Z","shell.execute_reply":"2025-09-22T16:41:10.926582Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"## Model Building","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras import layers, models, optimizers, losses, metrics\n\n# LSTM baseline model\nmodel = models.Sequential([\n    layers.LSTM(64, return_sequences=False, input_shape=input_shape),  # LSTM instead of GRU\n    layers.Dense(64, activation='relu'),\n    layers.Dense(1, activation='sigmoid')\n])\n\nmodel.compile(\n    optimizer='adam',\n    loss='binary_crossentropy',\n    metrics=['accuracy']\n)\n\nhistory = model.fit(\n    X_train, y_train,\n    epochs=20,\n    batch_size=256,\n    validation_data=(X_val, y_val),\n    verbose=1\n)\n\n# Predictions\ny_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n\n# Evaluate with your custom function\nnormal_results = evaluate_metrics(y_true=y_test, y_pred=y_pred)\nprint(f'Baseline LSTM results: {normal_results}')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-22T16:41:10.928080Z","iopub.execute_input":"2025-09-22T16:41:10.928731Z","iopub.status.idle":"2025-09-22T16:42:25.559509Z","shell.execute_reply.started":"2025-09-22T16:41:10.928702Z","shell.execute_reply":"2025-09-22T16:42:25.558948Z"}},"outputs":[{"name":"stderr","text":"I0000 00:00:1758559272.109074      36 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\nI0000 00:00:1758559272.109772      36 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(**kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/20\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1758559276.258678     114 cuda_dnn.cc:529] Loaded cuDNN version 90300\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 8ms/step - accuracy: 0.6715 - loss: 0.5485 - val_accuracy: 0.9179 - val_loss: 0.2214\nEpoch 2/20\n\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9187 - loss: 0.2094 - val_accuracy: 0.9045 - val_loss: 0.2440\nEpoch 3/20\n\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9194 - loss: 0.1920 - val_accuracy: 0.9211 - val_loss: 0.1681\nEpoch 4/20\n\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9331 - loss: 0.1642 - val_accuracy: 0.9437 - val_loss: 0.1450\nEpoch 5/20\n\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9434 - loss: 0.1442 - val_accuracy: 0.9512 - val_loss: 0.1244\nEpoch 6/20\n\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9499 - loss: 0.1297 - val_accuracy: 0.9519 - val_loss: 0.1389\nEpoch 7/20\n\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9501 - loss: 0.1268 - val_accuracy: 0.9589 - val_loss: 0.1085\nEpoch 8/20\n\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9573 - loss: 0.1104 - val_accuracy: 0.9611 - val_loss: 0.1032\nEpoch 9/20\n\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9645 - loss: 0.0969 - val_accuracy: 0.9535 - val_loss: 0.1157\nEpoch 10/20\n\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9630 - loss: 0.0986 - val_accuracy: 0.9524 - val_loss: 0.1103\nEpoch 11/20\n\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9666 - loss: 0.0907 - val_accuracy: 0.9711 - val_loss: 0.0824\nEpoch 12/20\n\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9691 - loss: 0.0849 - val_accuracy: 0.9692 - val_loss: 0.0849\nEpoch 13/20\n\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9641 - loss: 0.0993 - val_accuracy: 0.9732 - val_loss: 0.0785\nEpoch 14/20\n\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9719 - loss: 0.0781 - val_accuracy: 0.9713 - val_loss: 0.0787\nEpoch 15/20\n\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9717 - loss: 0.0790 - val_accuracy: 0.9767 - val_loss: 0.0660\nEpoch 16/20\n\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9737 - loss: 0.0745 - val_accuracy: 0.9755 - val_loss: 0.0694\nEpoch 17/20\n\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9748 - loss: 0.0695 - val_accuracy: 0.9786 - val_loss: 0.0641\nEpoch 18/20\n\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9768 - loss: 0.0665 - val_accuracy: 0.9750 - val_loss: 0.0736\nEpoch 19/20\n\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9751 - loss: 0.0707 - val_accuracy: 0.9791 - val_loss: 0.0608\nEpoch 20/20\n\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9758 - loss: 0.0679 - val_accuracy: 0.9782 - val_loss: 0.0597\n\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\nBaseline LSTM results: {'f1': 0.9810269799825936, 'precision': 0.9764379764379765, 'recall': 0.9856593214410633, 'mcc': 0.963306497032815, 'specificity': 0.9779364049318306, 'balanced_accuracy': 0.981797863186447}\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"## Attack codes","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\n\n@tf.function\ndef fgsm_attack_tf(model, inputs, labels, epsilon=0.05, clip_min=-1.5, clip_max=1.5):\n    inputs = tf.convert_to_tensor(inputs, dtype=tf.float32)\n    labels = tf.reshape(tf.convert_to_tensor(labels, dtype=tf.float32), (-1, 1))\n\n    with tf.GradientTape() as tape:\n        tape.watch(inputs)\n        predictions = model(inputs, training=False)\n        loss = tf.keras.losses.binary_crossentropy(labels, predictions)\n        loss = tf.reduce_mean(loss)\n\n    gradients = tape.gradient(loss, inputs)\n    signed_grad = tf.sign(gradients)\n    x_adv = inputs + epsilon * signed_grad\n    x_adv = tf.clip_by_value(x_adv, clip_min, clip_max)\n    return x_adv\n\n\n@tf.function\ndef pgd_attack_tf(model, inputs, labels, epsilon=0.05, alpha=0.01, num_iter=10, clip_min=-1.5, clip_max=1.5):\n    inputs = tf.convert_to_tensor(inputs, dtype=tf.float32)\n    labels = tf.reshape(tf.convert_to_tensor(labels, dtype=tf.float32), (-1, 1))\n\n    x_adv = tf.identity(inputs)\n\n    for _ in tf.range(num_iter):\n        with tf.GradientTape() as tape:\n            tape.watch(x_adv)\n            predictions = model(x_adv, training=False)\n            loss = tf.keras.losses.binary_crossentropy(labels, predictions)\n            loss = tf.reduce_mean(loss)\n\n        gradients = tape.gradient(loss, x_adv)\n        x_adv = x_adv + alpha * tf.sign(gradients)\n        x_adv = tf.clip_by_value(x_adv, inputs - epsilon, inputs + epsilon)\n        x_adv = tf.clip_by_value(x_adv, clip_min, clip_max)\n\n    return x_adv\n\n\n@tf.function\ndef mi_fgsm_attack(model, inputs, labels, epsilon=0.05, alpha=0.01, steps=40, decay=1.0, clip_min=-1.5, clip_max=1.5):\n    inputs = tf.convert_to_tensor(inputs, dtype=tf.float32)\n    labels = tf.reshape(tf.convert_to_tensor(labels, dtype=tf.float32), (-1, 1))\n\n    x_adv = tf.identity(inputs)\n    g = tf.zeros_like(inputs)\n\n    for _ in tf.range(steps):\n        with tf.GradientTape() as tape:\n            tape.watch(x_adv)\n            logits = model(x_adv, training=False)\n            loss = tf.keras.losses.binary_crossentropy(labels, logits)\n            loss = tf.reduce_mean(loss)\n\n        grad = tape.gradient(loss, x_adv)\n        # Normalize gradient\n        grad_norm = tf.reduce_mean(tf.abs(grad), axis=list(range(1, len(grad.shape))), keepdims=True)\n        grad = grad / (grad_norm + 1e-8)\n\n        g = decay * g + grad\n        x_adv = x_adv + alpha * tf.sign(g)\n        x_adv = tf.clip_by_value(x_adv, inputs - epsilon, inputs + epsilon)\n        x_adv = tf.clip_by_value(x_adv, clip_min, clip_max)\n\n    return x_adv\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-22T16:42:25.560331Z","iopub.execute_input":"2025-09-22T16:42:25.560558Z","iopub.status.idle":"2025-09-22T16:42:25.572431Z","shell.execute_reply.started":"2025-09-22T16:42:25.560530Z","shell.execute_reply":"2025-09-22T16:42:25.571783Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def evaluate_model(model, X, y,epsilon_list=[0.01, 0.03, 0.05, 0.07]):\n    import tensorflow as tf\n    import numpy as np\n\n    X_clean = X\n    x_clean_tf = tf.convert_to_tensor(X_clean, dtype=tf.float32)\n    y_tf = tf.convert_to_tensor(y, dtype=tf.float32)\n\n    # ---------------- Clean Evaluation ----------------\n    print(\"✅ Clean Evaluation:\")\n    y_pred_clean = (model.predict(x_clean_tf) > 0.5).astype(int).flatten()\n    clean_metrics = evaluate_metrics(y, y_pred_clean)\n    print(f\"  F1: {clean_metrics['f1']:.4f}, Precision: {clean_metrics['precision']:.4f}, Recall: {clean_metrics['recall']:.4f}, MCC: {clean_metrics.get('mcc', 'N/A')}, Specificity: {clean_metrics.get('specificity', 'N/A')}, Balanced Acc: {clean_metrics.get('balanced_accuracy', 'N/A')}\")\n\n    # ---------------- FGSM Evaluation ----------------\n    print(\"\\nFGSM Attack Evaluation\")\n    fgsm_results = []\n    for eps in epsilon_list:\n        x_adv = fgsm_attack_tf(model, x_clean_tf, y_tf, epsilon=eps )\n        x_adv_tf = tf.convert_to_tensor(x_adv.numpy(), dtype=tf.float32)\n        y_pred_adv = (model.predict(x_adv_tf) > 0.5).astype(int).flatten()\n\n        adv_metrics = evaluate_metrics(y, y_pred_adv)\n        fgsm_results.append({'epsilon': eps, 'adversarial': adv_metrics})\n\n        print(f\"  FGSM ε={eps:.2f} → F1: {adv_metrics['f1']:.4f}, Precision: {adv_metrics['precision']:.4f}, Recall: {adv_metrics['recall']:.4f}, MCC: {adv_metrics.get('mcc', 'N/A')}, Specificity: {adv_metrics.get('specificity', 'N/A')}, Balanced Acc: {adv_metrics.get('balanced_accuracy', 'N/A')}\")\n\n    # ---------------- PGD Evaluation ----------------\n    print(\"\\nPGD Attack Evaluation:\")\n    pgd_results = []\n    for eps in epsilon_list:\n        alpha = eps * 0.2\n        x_adv = pgd_attack_tf(model, x_clean_tf, y_tf, epsilon=eps, alpha=alpha, num_iter=10)\n        x_adv_tf = tf.convert_to_tensor(x_adv.numpy(), dtype=tf.float32)\n        y_pred_adv = (model.predict(x_adv_tf) > 0.5).astype(int).flatten()\n\n        adv_metrics = evaluate_metrics(y, y_pred_adv)\n        pgd_results.append({'epsilon': eps, 'adversarial': adv_metrics})\n\n        print(f\"  PGD ε={eps:.2f} → F1: {adv_metrics['f1']:.4f}, Precision: {adv_metrics['precision']:.4f}, Recall: {adv_metrics['recall']:.4f}, MCC: {adv_metrics.get('mcc', 'N/A')}, Specificity: {adv_metrics.get('specificity', 'N/A')}, Balanced Acc: {adv_metrics.get('balanced_accuracy', 'N/A')}\")\n\n    # ---------------- MI-FGSM Evaluation ----------------\n    print(\"\\nMI-FGSM Attack Evaluation:\")\n    mifgsm_results = []\n    for eps in epsilon_list:\n        alpha = eps * 0.2\n        steps = 10   \n        decay = 1.0   \n\n        x_adv = mi_fgsm_attack(model, x_clean_tf, y_tf, epsilon=eps, alpha=alpha, steps=steps, decay=decay)\n        x_adv_tf = tf.convert_to_tensor(x_adv.numpy(), dtype=tf.float32)\n        y_pred_adv = (model.predict(x_adv_tf) > 0.5).astype(int).flatten()\n\n        adv_metrics = evaluate_metrics(y, y_pred_adv)\n        mifgsm_results.append({'epsilon': eps, 'adversarial': adv_metrics})\n\n        print(f\" MI-FGSM ε={eps:.2f} → F1: {adv_metrics['f1']:.4f}, Precision: {adv_metrics['precision']:.4f}, Recall: {adv_metrics['recall']:.4f}, MCC: {adv_metrics.get('mcc', 'N/A')}, Specificity: {adv_metrics.get('specificity', 'N/A')}, Balanced Acc: {adv_metrics.get('balanced_accuracy', 'N/A')}\")\n\n    return fgsm_results, pgd_results, mifgsm_results\n        \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-22T16:42:25.574398Z","iopub.execute_input":"2025-09-22T16:42:25.574624Z","iopub.status.idle":"2025-09-22T16:42:25.590245Z","shell.execute_reply.started":"2025-09-22T16:42:25.574595Z","shell.execute_reply":"2025-09-22T16:42:25.589641Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"print(\"Results: \")\nepsilons = [.01,.03,.05,.07]\nfgsm_results, pgd_results, mifgsm_results = evaluate_model(\n    model=model,\n    X=X_test,\n    y=y_test,\n    epsilon_list=epsilons,\n)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-22T16:42:25.591013Z","iopub.execute_input":"2025-09-22T16:42:25.591716Z","iopub.status.idle":"2025-09-22T16:42:47.889306Z","shell.execute_reply.started":"2025-09-22T16:42:25.591689Z","shell.execute_reply":"2025-09-22T16:42:47.888697Z"}},"outputs":[{"name":"stdout","text":"Results: \n✅ Clean Evaluation:\n\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n  F1: 0.9810, Precision: 0.9764, Recall: 0.9857, MCC: 0.963306497032815, Specificity: 0.9779364049318306, Balanced Acc: 0.981797863186447\n\nFGSM Attack Evaluation\n\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n  FGSM ε=0.01 → F1: 0.9325, Precision: 0.9186, Recall: 0.9468, MCC: 0.8683778485451822, Specificity: 0.9221284879947786, Balanced Acc: 0.9344815227661896\n\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n  FGSM ε=0.03 → F1: 0.6596, Precision: 0.5445, Recall: 0.8363, MCC: 0.21320256215643882, Specificity: 0.35107073329006, Balanced Acc: 0.5936885670647571\n\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n  FGSM ε=0.05 → F1: 0.6205, Precision: 0.5133, Recall: 0.7842, MCC: 0.10687856939478405, Specificity: 0.3101881894873358, Balanced Acc: 0.5471892329038638\n\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n  FGSM ε=0.07 → F1: 0.6540, Precision: 0.5588, Recall: 0.7884, MCC: 0.22564937780382047, Specificity: 0.4224529526281498, Balanced Acc: 0.6054202503609445\n\nPGD Attack Evaluation:\n\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n  PGD ε=0.01 → F1: 0.9116, Precision: 0.8946, Recall: 0.9293, MCC: 0.8272051514373847, Specificity: 0.898442569759867, Balanced Acc: 0.9138942474542602\n\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n  PGD ε=0.03 → F1: 0.5864, Precision: 0.4832, Recall: 0.7457, MCC: 0.006784065209506105, Specificity: 0.2602206359506729, Balanced Acc: 0.5029679605076904\n\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n  PGD ε=0.05 → F1: 0.4708, Precision: 0.3785, Recall: 0.6226, MCC: -0.400817250792713, Specificity: 0.05158987670343765, Balanced Acc: 0.33709259487497867\n\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n  PGD ε=0.07 → F1: 0.3405, Precision: 0.2858, Recall: 0.4211, MCC: -0.6103342698350391, Specificity: 0.02401038286826658, Balanced Acc: 0.22256832539705737\n\nMI-FGSM Attack Evaluation:\n\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n MI-FGSM ε=0.01 → F1: 0.9160, Precision: 0.8968, Recall: 0.9360, MCC: 0.8354759446944875, Specificity: 0.9000648929266418, Balanced Acc: 0.9180282491915476\n\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n MI-FGSM ε=0.03 → F1: 0.5637, Precision: 0.4466, Recall: 0.7643, MCC: -0.14995803803616548, Specificity: 0.12134977287475271, Balanced Acc: 0.4428015041358723\n\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n MI-FGSM ε=0.05 → F1: 0.4686, Precision: 0.3746, Recall: 0.6254, MCC: -0.43138922750778463, Specificity: 0.03147306943543052, Balanced Acc: 0.3284332818320909\n\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n MI-FGSM ε=0.07 → F1: 0.3865, Precision: 0.3226, Recall: 0.4820, MCC: -0.5080530373168708, Specificity: 0.06099935107073131, Balanced Acc: 0.27149302985505785\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"## Adversarial Training","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import Model\n\nclass PGDAdversarialTrainerCNN(tf.keras.Model):\n    def __init__(self, base_model, epsilon_range=(0.01, 0.1), alpha=0.01, num_iter=7, clip_min=-1.5, clip_max=1.5):\n        super().__init__()\n        self.base_model = base_model\n        self.epsilon_range = epsilon_range\n        self.alpha = alpha\n        self.num_iter = num_iter\n        self.clip_min = clip_min\n        self.clip_max = clip_max\n        self.loss_fn = tf.keras.losses.BinaryCrossentropy()\n        self.metric = tf.keras.metrics.BinaryAccuracy()\n\n    def compile(self, optimizer, **kwargs):\n        super().compile(**kwargs)\n        self.optimizer = optimizer\n\n    def pgd_attack_batch(self, x, y):\n        x_adv = tf.identity(x)\n        y = tf.expand_dims(tf.cast(y, tf.float32), axis=-1)\n        epsilon = tf.random.uniform([], *self.epsilon_range)\n\n        for _ in range(self.num_iter):\n            with tf.GradientTape() as tape:\n                tape.watch(x_adv)\n                preds = self.base_model(x_adv, training=True)\n                loss = self.loss_fn(y, preds)\n\n            grad = tape.gradient(loss, x_adv)\n            x_adv = x_adv + self.alpha * tf.sign(grad)\n            # Project back to epsilon ball around original input\n            x_adv = tf.clip_by_value(x_adv, x - epsilon, x + epsilon)\n            x_adv = tf.clip_by_value(x_adv, self.clip_min, self.clip_max)\n\n        return x_adv\n\n    def train_step(self, data):\n        x, y = data\n        x_adv = self.pgd_attack_batch(x, y)\n\n        # Combine clean and adversarial samples\n        x_combined = tf.concat([x, x_adv], axis=0)\n        y_combined = tf.concat([y, y], axis=0)\n        y_combined = tf.expand_dims(tf.cast(y_combined, tf.float32), axis=-1)\n\n        with tf.GradientTape() as tape:\n            preds = self.base_model(x_combined, training=True)\n            loss = self.loss_fn(y_combined, preds)\n\n        grads = tape.gradient(loss, self.base_model.trainable_variables)\n        self.optimizer.apply_gradients(zip(grads, self.base_model.trainable_variables))\n        self.metric.update_state(y_combined, preds)\n\n        return {\"loss\": loss, \"accuracy\": self.metric.result()}\n\n    def test_step(self, data):\n        x, y = data\n        y = tf.expand_dims(tf.cast(y, tf.float32), axis=-1)\n        preds = self.base_model(x, training=False)\n        loss = self.loss_fn(y, preds)\n        self.metric.update_state(y, preds)\n        return {\"loss\": loss, \"accuracy\": self.metric.result()}\n\n    def call(self, inputs):\n        return self.base_model(inputs)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-22T16:42:47.890027Z","iopub.execute_input":"2025-09-22T16:42:47.890216Z","iopub.status.idle":"2025-09-22T16:42:47.900857Z","shell.execute_reply.started":"2025-09-22T16:42:47.890200Z","shell.execute_reply":"2025-09-22T16:42:47.900075Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Wrap your existing CNN\n\nfrom tensorflow.keras.callbacks import EarlyStopping\n\nearly_stop = EarlyStopping(\n    monitor='val_loss',   # monitor validation loss\n    patience=3,           # stop if no improvement for 3 epochs\n    restore_best_weights=True\n)\n\n\nadv_model = PGDAdversarialTrainerCNN(\n    base_model=model,           \n    epsilon_range=(0.01, 0.05),\n    alpha=0.01,\n    num_iter=7,\n    clip_min=-1.5,\n    clip_max=1.5\n)\n\nadv_model.compile(optimizer=tf.keras.optimizers.Adam())\n\n# Train with adversarial training\nadv_model.fit(\n    x=X_train,\n    y=y_train,\n    validation_data=(X_val, y_val),\n    batch_size=256,\n    epochs=20,\n    callbacks=[early_stop] \n\n)\n\nprint(\"Advesarial training Results: \")\nepsilons = [.01,.03,.05,.07]\nfgsm_results, pgd_results, mifgsm_results = evaluate_model(\n    model=adv_model,\n    X=X_test,\n    y=y_test,\n    epsilon_list=epsilons,\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-22T16:42:47.901531Z","iopub.execute_input":"2025-09-22T16:42:47.901719Z","iopub.status.idle":"2025-09-22T16:47:53.186993Z","shell.execute_reply.started":"2025-09-22T16:42:47.901703Z","shell.execute_reply":"2025-09-22T16:47:53.186309Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/20\n\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 39ms/step - accuracy: 0.8851 - loss: 0.2579 - val_accuracy: 0.9444 - val_loss: 0.1357\nEpoch 2/20\n\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 39ms/step - accuracy: 0.9239 - loss: 0.1891 - val_accuracy: 0.9465 - val_loss: 0.1029\nEpoch 3/20\n\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 40ms/step - accuracy: 0.9398 - loss: 0.1501 - val_accuracy: 0.9661 - val_loss: 0.0789\nEpoch 4/20\n\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 40ms/step - accuracy: 0.9473 - loss: 0.1278 - val_accuracy: 0.9727 - val_loss: 0.0561\nEpoch 5/20\n\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 40ms/step - accuracy: 0.9557 - loss: 0.1123 - val_accuracy: 0.9740 - val_loss: 0.0566\nEpoch 6/20\n\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 39ms/step - accuracy: 0.9599 - loss: 0.0988 - val_accuracy: 0.9750 - val_loss: 0.0509\nEpoch 7/20\n\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 39ms/step - accuracy: 0.9631 - loss: 0.0927 - val_accuracy: 0.9779 - val_loss: 0.0507\nEpoch 8/20\n\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 40ms/step - accuracy: 0.9661 - loss: 0.0890 - val_accuracy: 0.9737 - val_loss: 0.0493\nEpoch 9/20\n\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 40ms/step - accuracy: 0.9664 - loss: 0.0845 - val_accuracy: 0.9792 - val_loss: 0.0391\nEpoch 10/20\n\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 39ms/step - accuracy: 0.9697 - loss: 0.0798 - val_accuracy: 0.9790 - val_loss: 0.0595\nEpoch 11/20\n\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 40ms/step - accuracy: 0.9706 - loss: 0.0778 - val_accuracy: 0.9789 - val_loss: 0.0440\nEpoch 12/20\n\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 40ms/step - accuracy: 0.9720 - loss: 0.0743 - val_accuracy: 0.9779 - val_loss: 0.0385\nEpoch 13/20\n\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 39ms/step - accuracy: 0.9737 - loss: 0.0720 - val_accuracy: 0.9806 - val_loss: 0.0461\nEpoch 14/20\n\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 40ms/step - accuracy: 0.9733 - loss: 0.0697 - val_accuracy: 0.9819 - val_loss: 0.0394\nEpoch 15/20\n\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 40ms/step - accuracy: 0.9752 - loss: 0.0657 - val_accuracy: 0.9818 - val_loss: 0.0503\nAdvesarial training Results: \n✅ Clean Evaluation:\n\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n  F1: 0.9766, Precision: 0.9889, Recall: 0.9647, MCC: 0.95572750958355, Specificity: 0.989941596365964, Balanced Acc: 0.9773072794701454\n\nFGSM Attack Evaluation\n\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n  FGSM ε=0.01 → F1: 0.9720, Precision: 0.9863, Recall: 0.9580, MCC: 0.9470426033981267, Specificity: 0.9876703439324793, Balanced Acc: 0.972848813099503\n\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n  FGSM ε=0.03 → F1: 0.9615, Precision: 0.9811, Recall: 0.9426, MCC: 0.927742442863177, Specificity: 0.9831278390655099, Balanced Acc: 0.9628825624148816\n\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n  FGSM ε=0.05 → F1: 0.9464, Precision: 0.9678, Recall: 0.9258, MCC: 0.8994942029565159, Specificity: 0.9714471122647316, Balanced Acc: 0.9486476554677978\n\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n  FGSM ε=0.07 → F1: 0.9247, Precision: 0.9352, Recall: 0.9143, MCC: 0.8564707520916588, Specificity: 0.9412719013627209, Balanced Acc: 0.9277888013284399\n\nPGD Attack Evaluation:\n\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n  PGD ε=0.01 → F1: 0.9720, Precision: 0.9863, Recall: 0.9580, MCC: 0.9470426033981267, Specificity: 0.9876703439324793, Balanced Acc: 0.972848813099503\n\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n  PGD ε=0.03 → F1: 0.9593, Precision: 0.9789, Recall: 0.9405, MCC: 0.9236883786594713, Specificity: 0.9811810512653802, Balanced Acc: 0.9608598505714799\n\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n  PGD ε=0.05 → F1: 0.9320, Precision: 0.9489, Recall: 0.9157, MCC: 0.8715541688791602, Specificity: 0.954250486696919, Balanced Acc: 0.9349776392910968\n\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n  PGD ε=0.07 → F1: 0.8905, Precision: 0.8979, Recall: 0.8832, MCC: 0.7906241089631894, Specificity: 0.9068786502270958, Balanced Acc: 0.8950272929344643\n\nMI-FGSM Attack Evaluation:\n\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n MI-FGSM ε=0.01 → F1: 0.9720, Precision: 0.9863, Recall: 0.9580, MCC: 0.9470426033981267, Specificity: 0.9876703439324793, Balanced Acc: 0.972848813099503\n\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n MI-FGSM ε=0.03 → F1: 0.9595, Precision: 0.9792, Recall: 0.9405, MCC: 0.9240371274748496, Specificity: 0.9815055158987351, Balanced Acc: 0.9610220828881574\n\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n MI-FGSM ε=0.05 → F1: 0.9340, Precision: 0.9511, Recall: 0.9175, MCC: 0.8752777487524942, Specificity: 0.9561972744970488, Balanced Acc: 0.9368254648106091\n\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n MI-FGSM ε=0.07 → F1: 0.9006, Precision: 0.9059, Recall: 0.8954, MCC: 0.8094985338116113, Specificity: 0.9136924075275499, Balanced Acc: 0.9045551929208229\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# generate adversarial values for gan training\n\n# experimental PGD code (GPU-optimized, batched)\ndef pgd_attack_tf_fast(model, x, y, epsilon=0.05, alpha=0.01, num_iter=10, clip_min=-1.5, clip_max=1.5):\n    x = tf.convert_to_tensor(x, dtype=tf.float32)\n    y = tf.reshape(tf.cast(y, tf.float32), (-1, 1))\n    x_orig = tf.identity(x)\n    x_adv = tf.identity(x)\n\n    for _ in range(num_iter):\n        with tf.GradientTape() as tape:\n            tape.watch(x_adv)\n            logits = model(x_adv, training=False)\n            loss = tf.keras.losses.binary_crossentropy(y, logits)\n            loss = tf.reduce_mean(loss)\n\n        grad = tape.gradient(loss, x_adv)\n        signed_grad = tf.sign(grad)\n        x_adv = x_adv + alpha * signed_grad\n        x_adv = tf.clip_by_value(x_adv, x_orig - epsilon, x_orig + epsilon)\n        x_adv = tf.clip_by_value(x_adv, clip_min, clip_max)\n\n    return x_adv\n\ndef batch_pgd_attack(model, X, y, attack_fn, batch_size=1024, **kwargs):\n    adv_list = []\n    X = tf.convert_to_tensor(X, dtype=tf.float32)\n    y = tf.convert_to_tensor(y, dtype=tf.float32)\n    for i in range(0, len(X), batch_size):\n        X_batch = X[i:i+batch_size]\n        y_batch = y[i:i+batch_size]\n        x_adv_batch = attack_fn(model, X_batch, y_batch, **kwargs)\n        adv_list.append(x_adv_batch)\n    return tf.concat(adv_list, axis=0)\n\n# Generate adversarial examples in batches to leverage GPU\nX_adv_train = batch_pgd_attack(model, X_train, y_train, pgd_attack_tf_fast, epsilon=0.05, batch_size=1024)\nprint(f\"X_adv_train shape: {X_adv_train.shape}\")\n\nX_adv_val = batch_pgd_attack(model, X_val, y_val, pgd_attack_tf_fast, epsilon=0.05, batch_size=1024)\nprint(f\"X_adv_val shape: {X_adv_val.shape}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-22T16:47:53.187704Z","iopub.execute_input":"2025-09-22T16:47:53.187969Z","iopub.status.idle":"2025-09-22T16:48:29.531266Z","shell.execute_reply.started":"2025-09-22T16:47:53.187940Z","shell.execute_reply":"2025-09-22T16:48:29.530630Z"}},"outputs":[{"name":"stdout","text":"X_adv_train shape: (118813, 84, 1)\nX_adv_val shape: (23763, 84, 1)\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, Model\nfrom tensorflow.keras.callbacks import EarlyStopping\n\n# ------------------------------\n# Generator CNN (1D output)\n# ------------------------------\nclass GeneratorCNN(Model):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = layers.Conv1D(64, 3, padding='same', activation='relu')\n        self.bn1 = layers.BatchNormalization()\n        self.conv2 = layers.Conv1D(128, 3, padding='same', activation='relu')\n        self.bn2 = layers.BatchNormalization()\n        self.conv3 = layers.Conv1D(64, 3, padding='same', activation='relu')\n        self.bn3 = layers.BatchNormalization()\n        # final output: 1 channel (same as clean input)\n        self.output_layer = layers.Conv1D(1, 3, padding='same', activation='linear')\n\n    def call(self, x, training=False):\n        x = self.conv1(x)\n        x = self.bn1(x, training=training)\n        x = self.conv2(x)\n        x = self.bn2(x, training=training)\n        x = self.conv3(x)\n        x = self.bn3(x, training=training)\n        return self.output_layer(x)\n\n\n# ------------------------------\n# Discriminator CNN (1D input)\n# ------------------------------\nclass DiscriminatorCNN(Model):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = layers.Conv1D(64, 3, padding='same', activation='relu')\n        self.global_pool = layers.GlobalAveragePooling1D()\n        self.dense1 = layers.Dense(64, activation='relu')\n        self.output_layer = layers.Dense(1, activation='sigmoid')\n\n    def call(self, x, training=False):\n        x = self.conv1(x)\n        x = self.global_pool(x)\n        x = self.dense1(x)\n        return self.output_layer(x)\n\n\n# ------------------------------\n# GAN Purifier CNN\n# ------------------------------\nclass GANPurifierCNN(Model):\n    def __init__(self, generator, discriminator, lambda_adv=0.5):\n        super().__init__()\n        self.generator = generator\n        self.discriminator = discriminator\n        self.lambda_adv = lambda_adv\n        self.bce = tf.keras.losses.BinaryCrossentropy()\n        self.l1 = tf.keras.losses.MeanAbsoluteError()\n        self.d_optimizer = tf.keras.optimizers.Adam(1e-4)\n        self.g_optimizer = tf.keras.optimizers.Adam(1e-4)\n\n    def compile(self, **kwargs):\n        super().compile(**kwargs)\n\n    def train_step(self, data):\n        x_clean, x_adv = data\n        with tf.GradientTape(persistent=True) as tape:\n            x_fake = self.generator(x_adv, training=True)\n            real_pred = self.discriminator(x_clean, training=True)\n            fake_pred = self.discriminator(x_fake, training=True)\n\n            # Discriminator loss\n            d_loss = self.bce(tf.ones_like(real_pred), real_pred) + \\\n                     self.bce(tf.zeros_like(fake_pred), fake_pred)\n\n            # Generator loss (reconstruction + adversarial)\n            g_loss = self.l1(x_clean, x_fake) + \\\n                     self.lambda_adv * self.bce(tf.ones_like(fake_pred), fake_pred)\n\n            # Cosine similarity\n            x_clean_flat = tf.reshape(x_clean, (tf.shape(x_clean)[0], -1))\n            x_fake_flat = tf.reshape(x_fake, (tf.shape(x_fake)[0], -1))\n            cos_sim = -tf.reduce_mean(\n                tf.keras.losses.cosine_similarity(x_clean_flat, x_fake_flat)\n            )\n\n        # Compute gradients\n        d_grads = tape.gradient(d_loss, self.discriminator.trainable_variables)\n        g_grads = tape.gradient(g_loss, self.generator.trainable_variables)\n\n        # Apply gradients\n        self.d_optimizer.apply_gradients(zip(d_grads, self.discriminator.trainable_variables))\n        self.g_optimizer.apply_gradients(zip(g_grads, self.generator.trainable_variables))\n\n        return {\"d_loss\": d_loss, \"g_loss\": g_loss,\n                \"reconstruction_loss\": self.l1(x_clean, x_fake),\n                \"cosine_similarity\": cos_sim}\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-22T16:48:29.532204Z","iopub.execute_input":"2025-09-22T16:48:29.532458Z","iopub.status.idle":"2025-09-22T16:48:29.545340Z","shell.execute_reply.started":"2025-09-22T16:48:29.532441Z","shell.execute_reply":"2025-09-22T16:48:29.544491Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# ------------------------------\n# Instantiate models\n# ------------------------------\ngenerator = GeneratorCNN()\ndiscriminator = DiscriminatorCNN()\n\n# Build models with dummy input (1 channel)\ndummy_input = tf.zeros((1, 95, 1))\ngenerator(dummy_input)\ndiscriminator(dummy_input)\n\ngan_cnn = GANPurifierCNN(generator, discriminator, lambda_adv=0.5)\ngan_cnn.compile()\n\n# ------------------------------\n# Prepare dataset\n# ------------------------------\nbatch_size = 512\ntrain_dataset = tf.data.Dataset.from_tensor_slices((X_train, X_adv_train))\ntrain_dataset = train_dataset.shuffle(10000).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n\nearly_stop = EarlyStopping(\n    monitor=\"cosine_similarity\",\n    mode=\"max\",              # higher cosine similarity is better\n    patience=10,\n    min_delta=1e-4,\n    restore_best_weights=True,\n    verbose=1\n)\n\n# ------------------------------\n# Train\n# ------------------------------\ngan_cnn.fit(train_dataset, epochs=100, callbacks=[early_stop])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-22T16:48:29.546104Z","iopub.execute_input":"2025-09-22T16:48:29.546273Z","iopub.status.idle":"2025-09-22T16:50:23.940467Z","shell.execute_reply.started":"2025-09-22T16:48:29.546260Z","shell.execute_reply":"2025-09-22T16:50:23.939704Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/100\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1758559716.185000     115 service.cc:148] XLA service 0x7de2a4b90230 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1758559716.185888     115 service.cc:156]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\nI0000 00:00:1758559716.185915     115 service.cc:156]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m 13/233\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - cosine_similarity: 0.1471 - d_loss: 1.3783 - g_loss: 0.8781 - reconstruction_loss: 0.5254 ","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1758559720.954131     115 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 29ms/step - cosine_similarity: 0.8382 - d_loss: 1.3798 - g_loss: 0.4612 - reconstruction_loss: 0.1124\nEpoch 2/100\n\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - cosine_similarity: 0.9774 - d_loss: 1.3814 - g_loss: 0.3845 - reconstruction_loss: 0.0363\nEpoch 3/100\n\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - cosine_similarity: 0.9851 - d_loss: 1.3810 - g_loss: 0.3766 - reconstruction_loss: 0.0285\nEpoch 4/100\n\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - cosine_similarity: 0.9882 - d_loss: 1.3805 - g_loss: 0.3730 - reconstruction_loss: 0.0247\nEpoch 5/100\n\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - cosine_similarity: 0.9899 - d_loss: 1.3795 - g_loss: 0.3712 - reconstruction_loss: 0.0228\nEpoch 6/100\n\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - cosine_similarity: 0.9910 - d_loss: 1.3780 - g_loss: 0.3703 - reconstruction_loss: 0.0216\nEpoch 7/100\n\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - cosine_similarity: 0.9918 - d_loss: 1.3761 - g_loss: 0.3696 - reconstruction_loss: 0.0204\nEpoch 8/100\n\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - cosine_similarity: 0.9924 - d_loss: 1.3717 - g_loss: 0.3705 - reconstruction_loss: 0.0202\nEpoch 9/100\n\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - cosine_similarity: 0.9926 - d_loss: 1.3639 - g_loss: 0.3727 - reconstruction_loss: 0.0205\nEpoch 10/100\n\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - cosine_similarity: 0.9932 - d_loss: 1.3538 - g_loss: 0.3745 - reconstruction_loss: 0.0195\nEpoch 11/100\n\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - cosine_similarity: 0.9934 - d_loss: 1.3391 - g_loss: 0.3781 - reconstruction_loss: 0.0194\nEpoch 12/100\n\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - cosine_similarity: 0.9938 - d_loss: 1.3265 - g_loss: 0.3803 - reconstruction_loss: 0.0181\nEpoch 13/100\n\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - cosine_similarity: 0.9938 - d_loss: 1.3004 - g_loss: 0.3882 - reconstruction_loss: 0.0187\nEpoch 14/100\n\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - cosine_similarity: 0.9936 - d_loss: 1.2618 - g_loss: 0.4002 - reconstruction_loss: 0.0197\nEpoch 15/100\n\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - cosine_similarity: 0.9940 - d_loss: 1.2672 - g_loss: 0.3966 - reconstruction_loss: 0.0175\nEpoch 16/100\n\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - cosine_similarity: 0.9941 - d_loss: 1.2414 - g_loss: 0.4045 - reconstruction_loss: 0.0175\nEpoch 17/100\n\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - cosine_similarity: 0.9941 - d_loss: 1.2256 - g_loss: 0.4089 - reconstruction_loss: 0.0172\nEpoch 18/100\n\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - cosine_similarity: 0.9940 - d_loss: 1.1938 - g_loss: 0.4198 - reconstruction_loss: 0.0176\nEpoch 19/100\n\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - cosine_similarity: 0.9940 - d_loss: 1.1716 - g_loss: 0.4271 - reconstruction_loss: 0.0174\nEpoch 20/100\n\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - cosine_similarity: 0.9938 - d_loss: 1.1443 - g_loss: 0.4374 - reconstruction_loss: 0.0181\nEpoch 21/100\n\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - cosine_similarity: 0.9939 - d_loss: 1.1327 - g_loss: 0.4410 - reconstruction_loss: 0.0175\nEpoch 22/100\n\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - cosine_similarity: 0.9939 - d_loss: 1.1235 - g_loss: 0.4437 - reconstruction_loss: 0.0171\nEpoch 23/100\n\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - cosine_similarity: 0.9939 - d_loss: 1.1046 - g_loss: 0.4503 - reconstruction_loss: 0.0171\nEpoch 24/100\n\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - cosine_similarity: 0.9938 - d_loss: 1.0867 - g_loss: 0.4570 - reconstruction_loss: 0.0171\nEpoch 25/100\n\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - cosine_similarity: 0.9936 - d_loss: 1.0685 - g_loss: 0.4639 - reconstruction_loss: 0.0174\nEpoch 26/100\n\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - cosine_similarity: 0.9936 - d_loss: 1.0504 - g_loss: 0.4714 - reconstruction_loss: 0.0173\nEpoch 27/100\n\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - cosine_similarity: 0.9932 - d_loss: 1.0203 - g_loss: 0.4877 - reconstruction_loss: 0.0181\nEpoch 28/100\n\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - cosine_similarity: 0.9932 - d_loss: 1.0137 - g_loss: 0.4859 - reconstruction_loss: 0.0178\nEpoch 28: early stopping\nRestoring model weights from the end of the best epoch: 18.\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.history.History at 0x7de2e0271b90>"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"# Generate purified outputs\nx_purified_val = generator(X_adv_val, training=False)\n\n# Cast to float32 for evaluation\nx_purified_val = tf.cast(x_purified_val, tf.float32)\nX_val_cast = tf.cast(X_val, tf.float32)\n\n# L1 (Mean Absolute Error)\nl1_loss = tf.reduce_mean(tf.abs(X_val_cast - x_purified_val))\n\n# L2 (Mean Squared Error)\nl2_loss = tf.reduce_mean(tf.square(X_val_cast - x_purified_val))\n\n# Normalize along sequence dimension\nX_val_norm = tf.nn.l2_normalize(tf.squeeze(X_val_cast, axis=-1), axis=1)           \nx_purified_norm = tf.nn.l2_normalize(tf.squeeze(x_purified_val, axis=-1), axis=1)  \n\n# Cosine similarity\ncos_sim = tf.reduce_mean(tf.reduce_sum(X_val_norm * x_purified_norm, axis=1))\n\nprint(f\"🧪 Purifier Evaluation Metrics:\")\nprint(f\"✅ Cosine Similarity: {cos_sim:.6f}\")\nprint(f\"   ➤ L1 Loss (MAE): {l1_loss:.6f}\")\nprint(f\"   ➤ L2 Loss (MSE): {l2_loss:.6f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-22T16:50:23.941406Z","iopub.execute_input":"2025-09-22T16:50:23.941945Z","iopub.status.idle":"2025-09-22T16:50:25.913775Z","shell.execute_reply.started":"2025-09-22T16:50:23.941919Z","shell.execute_reply":"2025-09-22T16:50:25.913161Z"}},"outputs":[{"name":"stdout","text":"🧪 Purifier Evaluation Metrics:\n✅ Cosine Similarity: 0.992974\n   ➤ L1 Loss (MAE): 0.023636\n   ➤ L2 Loss (MSE): 0.001513\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"## GAN+Adv Training evaluation","metadata":{}},{"cell_type":"code","source":"def evaluate_model(model, generator, adv_model, X, y, epsilon_list=[0.01, 0.03, 0.05, 0.07]):\n    import tensorflow as tf\n    import numpy as np\n\n    X_clean = X\n    x_clean_tf = tf.convert_to_tensor(X_clean, dtype=tf.float32)\n    y_tf = tf.convert_to_tensor(y, dtype=tf.float32)\n\n    # ----------- Clean Data Evaluation (no adversarial) -----------\n    print(\"✅ Clean Evaluation (GAN purified):\")\n    x_purified_clean = generator(x_clean_tf, training=False)\n    y_pred_clean_prob = adv_model.predict(x_purified_clean)\n    y_pred_clean = (y_pred_clean_prob > 0.5).astype(int).flatten()\n    clean_metrics = evaluate_metrics(y, y_pred_clean)\n    print(f\"Clean → F1: {clean_metrics['f1']:.4f}, Precision: {clean_metrics['precision']:.4f}, Recall: {clean_metrics['recall']:.4f}, MCC: {clean_metrics.get('mcc', 'N/A')}, Specificity: {clean_metrics.get('specificity', 'N/A')}, Balanced Acc: {clean_metrics.get('balanced_accuracy', 'N/A')}\")\n\n    # ----------- FGSM Attack Evaluation -----------\n    print(\"\\nFGSM Attack Evaluation with GAN Purification\")\n    fgsm_results = []\n    for eps in epsilon_list:\n        x_adv = fgsm_attack_tf(model, x_clean_tf, y_tf, epsilon=eps )\n        x_adv_tf = tf.convert_to_tensor(x_adv.numpy(), dtype=tf.float32)\n        x_purified = generator(x_adv_tf, training=False)\n        y_pred_prob = adv_model.predict(x_purified)\n        y_pred = (y_pred_prob > 0.5).astype(int).flatten()\n\n        adv_metrics = evaluate_metrics(y, y_pred)\n        fgsm_results.append({'epsilon': eps, 'adversarial': adv_metrics})\n\n        print(f\"  FGSM ε={eps:.2f} → F1: {adv_metrics['f1']:.4f}, Precision: {adv_metrics['precision']:.4f}, Recall: {adv_metrics['recall']:.4f}, MCC: {adv_metrics.get('mcc', 'N/A')}, Specificity: {adv_metrics.get('specificity', 'N/A')}, Balanced Acc: {adv_metrics.get('balanced_accuracy', 'N/A')}\")\n\n    # ----------- PGD Attack Evaluation -----------\n    print(\"\\nPGD Attack Evaluation with GAN Purification:\")\n    pgd_results = []\n    for eps in epsilon_list:\n        alpha = eps * 0.2\n        x_adv = pgd_attack_tf(model, x_clean_tf, y_tf, epsilon=eps, alpha=alpha, num_iter=10)\n        x_adv_tf = tf.convert_to_tensor(x_adv.numpy(), dtype=tf.float32)\n        x_purified = generator(x_adv_tf, training=False)\n        y_pred_prob = adv_model.predict(x_purified )\n        y_pred = (y_pred_prob > 0.5).astype(int).flatten()\n\n        adv_metrics = evaluate_metrics(y, y_pred)\n        pgd_results.append({'epsilon': eps, 'adversarial': adv_metrics})\n\n        print(f\"  PGD ε={eps:.2f} → F1: {adv_metrics['f1']:.4f}, Precision: {adv_metrics['precision']:.4f}, Recall: {adv_metrics['recall']:.4f}, MCC: {adv_metrics.get('mcc', 'N/A')}, Specificity: {adv_metrics.get('specificity', 'N/A')}, Balanced Acc: {adv_metrics.get('balanced_accuracy', 'N/A')}\")\n\n      # ----------- MI-FGSM Attack Evaluation -----------\n    print(\"\\nMI-FGSM Attack Evaluation with GAN Purification:\")\n    mifgsm_results = []\n    for eps in epsilon_list:\n        alpha = eps * 0.2\n        steps = 10\n        decay = 1.0\n\n        x_adv = mi_fgsm_attack(model, x_clean_tf, y_tf, epsilon=eps, alpha=alpha, steps=steps, decay=decay)\n        x_adv_tf = tf.convert_to_tensor(x_adv.numpy(), dtype=tf.float32)\n        x_purified = generator(x_adv_tf, training=False)\n        y_pred_prob = adv_model.predict(x_purified )\n        y_pred = (y_pred_prob > 0.5).astype(int).flatten()\n\n        adv_metrics = evaluate_metrics(y, y_pred)\n        mifgsm_results.append({'epsilon': eps, 'adversarial': adv_metrics})\n\n        print(f\"  MI-FGSM ε={eps:.2f} → F1: {adv_metrics['f1']:.4f}, Precision: {adv_metrics['precision']:.4f}, Recall: {adv_metrics['recall']:.4f}, MCC: {adv_metrics.get('mcc', 'N/A')}, Specificity: {adv_metrics.get('specificity', 'N/A')}, Balanced Acc: {adv_metrics.get('balanced_accuracy', 'N/A')}\")\n\n    return fgsm_results, pgd_results, mifgsm_results\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-22T16:50:25.914518Z","iopub.execute_input":"2025-09-22T16:50:25.914768Z","iopub.status.idle":"2025-09-22T16:50:25.925342Z","shell.execute_reply.started":"2025-09-22T16:50:25.914750Z","shell.execute_reply":"2025-09-22T16:50:25.924612Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"fgsm_full, pgd_full, mi_full = evaluate_model(model= model,generator=generator,adv_model= model, X = X_test, y = y_test)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-22T16:50:25.927379Z","iopub.execute_input":"2025-09-22T16:50:25.927584Z","iopub.status.idle":"2025-09-22T16:50:40.313613Z","shell.execute_reply.started":"2025-09-22T16:50:25.927568Z","shell.execute_reply":"2025-09-22T16:50:40.312857Z"}},"outputs":[{"name":"stdout","text":"✅ Clean Evaluation (GAN purified):\n\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\nClean → F1: 0.9665, Precision: 0.9741, Recall: 0.9591, MCC: 0.9360148689906431, Specificity: 0.976314081765056, Balanced Acc: 0.9676953409874598\n\nFGSM Attack Evaluation with GAN Purification\n\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n  FGSM ε=0.01 → F1: 0.9656, Precision: 0.9747, Recall: 0.9566, MCC: 0.9343671032628506, Specificity: 0.9769630110317659, Balanced Acc: 0.9667956013535884\n\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n  FGSM ε=0.03 → F1: 0.9577, Precision: 0.9702, Recall: 0.9454, MCC: 0.919633187647419, Specificity: 0.9730694354315064, Balanced Acc: 0.9592524511889956\n\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n  FGSM ε=0.05 → F1: 0.9521, Precision: 0.9665, Recall: 0.9381, MCC: 0.9092465287961075, Specificity: 0.9698247890979569, Balanced Acc: 0.953957515220542\n\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n  FGSM ε=0.07 → F1: 0.9138, Precision: 0.9101, Recall: 0.9175, MCC: 0.8331998579992984, Specificity: 0.9159636599610345, Balanced Acc: 0.9167086575426019\n\nPGD Attack Evaluation with GAN Purification:\n\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n  PGD ε=0.01 → F1: 0.9654, Precision: 0.9747, Recall: 0.9563, MCC: 0.9340345595848, Specificity: 0.9769630110317659, Balanced Acc: 0.9666207150296989\n\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n  PGD ε=0.03 → F1: 0.9584, Precision: 0.9709, Recall: 0.9461, MCC: 0.9209833186668062, Specificity: 0.9737183646982163, Balanced Acc: 0.9599266884701294\n\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n  PGD ε=0.05 → F1: 0.9535, Precision: 0.9686, Recall: 0.9388, MCC: 0.9119787510889228, Specificity: 0.9717715768980866, Balanced Acc: 0.9552806817683857\n\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n  PGD ε=0.07 → F1: 0.9339, Precision: 0.9411, Recall: 0.9269, MCC: 0.8736035776757185, Specificity: 0.9461388708630453, Balanced Acc: 0.936518193738623\n\nMI-FGSM Attack Evaluation with GAN Purification:\n\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n  MI-FGSM ε=0.01 → F1: 0.9656, Precision: 0.9750, Recall: 0.9563, MCC: 0.9343769329355824, Specificity: 0.9772874756651208, Balanced Acc: 0.9667829473463764\n\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n  MI-FGSM ε=0.03 → F1: 0.9582, Precision: 0.9702, Recall: 0.9465, MCC: 0.9206262953818165, Specificity: 0.9730694354315064, Balanced Acc: 0.959777110160664\n\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n  MI-FGSM ε=0.05 → F1: 0.9533, Precision: 0.9672, Recall: 0.9398, MCC: 0.9115874522205301, Specificity: 0.9704737183646668, Balanced Acc: 0.9551564114733442\n\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n  MI-FGSM ε=0.07 → F1: 0.9347, Precision: 0.9408, Recall: 0.9286, MCC: 0.874938529172108, Specificity: 0.9458144062296903, Balanced Acc: 0.9372303930413929\n","output_type":"stream"}],"execution_count":17}]}